% ============================================================================
% CHAPTER 2: COMPREHENSIVE LITERATURE REVIEW (EXPANDED)
% PhD Thesis: DFIG-Solar PV Interconnected Hybrid Energy Generation System
% ============================================================================

\section{Introduction to Literature Review}

This chapter reviews literature on DFIG-Solar PV hybrid systems and control strategies, tracing evolution from conventional to deep reinforcement learning approaches, emphasizing DDPG and TD3 applications in power systems. The review draws upon recent comprehensive surveys on AI in renewable power systems \cite{Li2024, Tang2024} and RL-based microgrid control \cite{Waghmare2025}.

% ============================================================================
\section{Evolution of Wind-Solar Hybrid Systems}

Hybrid wind-solar integration has evolved from simple parallel operation to sophisticated coordinated control over the past two decades.

\subsection{Early Hybrid Systems (2000--2010)}

Independent converters, separate MPPT, minimal coordination (Lasseter et al. CERTS microgrid). High costs, suboptimal efficiency limited deployment.

\subsection{Integrated Architectures (2010--2020)}

DC link integration of solar PV at DFIG converters (Tiwari et al., 2018) achieved 15--20\% converter reduction and 8--12\% efficiency improvement through unified voltage regulation and coordinated MPPT. Kumar et al. (2020) demonstrated dual MPPT achieving 8--15\% annual energy improvement. This architecture eliminates separate converters while maintaining independent power tracking, optimally balancing simplification and control flexibility.

\subsubsection{Battery Energy Storage Integration}

Bhattacharyya and Singh (2022) integrated BESS with DFIG-PV systems, achieving 45\% power fluctuation reduction, 25\% improved fault ride-through, and DSOSF-FLL control with < 20 ms settling. Recent work \cite{WindBattery2025} demonstrates lithium-ion integration achieving 60-75\% output variability reduction. IRENA \cite{IRENA_BESS_2025} reports 40\% cost decline since 2020, > 90\% round-trip efficiency, and 15-20 year lifespan. BESS provides power quality support, energy arbitrage, primary frequency response, and backup capability.

\subsubsection{Grid Stability and Frequency Regulation (2024--2025)}

Smahi et al. (2025) \cite{Smahi2025} review grid inertia solutions including VSM for synthetic inertia, fast-responding BESS, and AI-driven adaptive control. He et al. (2024) \cite{He2024} achieve 200 ms wind turbine FFR, 45\% better frequency nadir, and 35\% RoCoF reduction. MSESO-GADRC \cite{MSESO_GADRC_2025} demonstrates 60\% disturbance rejection improvement over PI with robustness to 40\% parameter variations.

\textbf{Stability Metrics:}
\begin{itemize}
    \item Frequency deviation: ±0.2 Hz standard (±0.05--0.10 Hz advanced)
    \item RoCoF: 1.0 Hz/s standard (0.3--0.5 Hz/s advanced)
    \item Voltage: ±5\% standard (±1--2\% advanced)
    \item FFR response: < 500 ms standard (150--250 ms advanced)
\end{itemize}

\subsection{Modern Smart Grid Integration (2020--Present)}

Recent developments include energy internet concepts (Yue and Han, 2019), multi-microgrid architectures (Hong et al., 2018), and prosumer-based systems (Zafar et al., 2018). Prasad et al. (2025) achieved 98.5\% MPPT efficiency with coordinated RSC-GSC control. GridIntegratedDFIG2025 demonstrated adaptive control achieving 2.60\% THD via OPAL-RT validation. Hybrid MPPT \cite{HybridMPPT2024} achieves 97.8--98.9\% efficiency versus 80--88\% conventional. Modern systems feature enhanced grid support, predictive control, ML-based optimization, and storage integration \cite{WindBattery2025}.

% ============================================================================
\section{Doubly Fed Induction Generator Technology}

\subsection{DFIG Fundamentals}

DFIG dominates variable-speed wind conversion with ~65\% market share (2024). The stator connects directly to the grid while the rotor uses back-to-back converters rated at 25--30\% of generator capacity, reducing cost versus full-scale converters in PMSG systems.

\subsubsection{Power Flow Equations}

\textbf{Stator Power:}
\begin{align}
P_s &= \frac{3}{2} V_s \left(\frac{L_m}{L_s} i_{dr} \sin\delta - i_{qr} \cos\delta \right) \\
Q_s &= \frac{3}{2} V_s \left(\frac{V_s}{\omega_s L_s} - \frac{L_m}{L_s} i_{dr} \cos\delta - i_{qr} \sin\delta \right)
\end{align}

\textbf{Rotor Power:} $P_r = s \cdot P_s$ where $s = \frac{\omega_s - \omega_r}{\omega_s}$

\subsection{d-q Reference Frame Modeling}

\textbf{Voltage Equations:}
\begin{align}
V_{qs} &= R_s i_{qs} + \frac{d\psi_{qs}}{dt} + \omega_s \psi_{ds}, \quad
V_{ds} = R_s i_{ds} + \frac{d\psi_{ds}}{dt} - \omega_s \psi_{qs} \\
V_{qr} &= R_r i_{qr} + \frac{d\psi_{qr}}{dt} + (\omega_s - \omega_r) \psi_{dr}, \quad
V_{dr} = R_r i_{dr} + \frac{d\psi_{dr}}{dt} - (\omega_s - \omega_r) \psi_{qr}
\end{align}

\textbf{Flux Linkages:}
\begin{align*}
\psi_{qs} = L_s i_{qs} + L_m i_{qr}, \quad \psi_{ds} = L_s i_{ds} + L_m i_{dr} \\
\psi_{qr} = L_r i_{qr} + L_m i_{qs}, \quad \psi_{dr} = L_r i_{dr} + L_m i_{ds}
\end{align*}

\subsection{Vector Control Principles}

Stator flux orientation ($\psi_{qs} = 0$) decouples active/reactive power control:
\begin{align}
P_s &\approx -\frac{3}{2} \frac{V_s L_m}{L_s} i_{qr} \\
Q_s &\approx \frac{3}{2} \frac{V_s}{L_s} \left( V_s - L_m i_{dr} \right)
\end{align}

Control objectives: MPPT, DC link regulation, grid synchronization, reactive power support, fault ride-through.

\subsection{Challenges in DFIG Control}

Wang et al. (2015) identified parameter variations, grid disturbances, mechanical dynamics, and sensor noise. Hu et al. (2019) addressed these via multi-agent systems achieving 15\% power quality improvement and 30\% stress reduction.

\subsection{Power Quality and Harmonic Mitigation}

MAO-RERNN \cite{MAO_RERNN_2025} achieves 1.56\% THD with 100 μs response time. Critical harmonics \cite{WindHarmonics2025} include 5th, 7th, 11th, and 13th orders.

\textbf{Mitigation Approaches:}
\begin{itemize}
    \item Passive filters: 3.5--5\% THD, low complexity
    \item Active PI: 2.5--3.5\% THD, 1--2 ms response
    \item Neuro-fuzzy: 2--2.5\% THD, 0.5--1 ms response
    \item MAO-RERNN: 1.5--1.8\% THD, < 0.1 ms response, very high complexity
\end{itemize}

\subsubsection{Power Quality Standards}

\textbf{IEEE 519-2014:} TDD < 5\%, harmonics 3rd/5th < 4\%, 7th < 2\%, 11th--17th < 1.5\%, voltage THD < 5\%.

\textbf{IEC 61000-3-6:} Voltage unbalance < 2\%, flicker Pst < 1.0 / Plt < 0.8.

\textbf{Quality Impacts:} Heating, resonance, equipment malfunction, power factor degradation.

% ============================================================================
\section{Solar PV Integration Architectures}

\subsection{Integration Topologies}

Independent: dedicated converters (simple control, high cost/losses). DC link integration: PV at DFIG DC link via boost + shared GSC, 15--25\% cost reduction. Challenges: coordination, stability, MPPT interactions. Nindra et al. (2019): ±2\% voltage, THD < 4\%. Preliminary real-time investigation of this architecture using OPAL-RT platform \cite{Pandey2022PIICON} confirmed feasibility of solar PV integration at the DFIG DC link with stable operation under varying wind and irradiance conditions.

\subsection{Maximum Power Point Tracking}

P\&O algorithm: perturbs voltage, observes power change, adjusts direction. Tracking: 95-98\%, simple but oscillates. Incremental Conductance ($\frac{dP}{dV} = 0 \Rightarrow \frac{dI}{dV} = -\frac{I}{V}$) achieves 97-99\% with less oscillation but higher complexity.

Advanced methods: Shuai et al. (2021) DRL-MPPT achieves 99.2\% with < 0.5s convergence and no steady-state oscillation. Kumar et al. (2024) unified controller achieves 98.5\% efficiency managing wind and solar simultaneously with reduced sensors.

\subsection{PV System Modeling}

\textbf{Single-Diode Model:}
\begin{equation}
I = I_{ph} - I_s \left( e^{\frac{V + IR_s}{n_s V_t}} - 1 \right) - \frac{V + IR_s}{R_p}
\end{equation}

\textbf{Temperature Effects:}
\begin{align*}
I_{ph}(T) &= I_{ph,ref} [ 1 + \alpha_I (T - T_{ref}) ] \\
V_{oc}(T) &= V_{oc,ref} [ 1 + \beta_V (T - T_{ref}) ]
\end{align*}
where $\alpha_I \approx +0.06\%/°C$ (current coefficient), $\beta_V \approx -0.3\%/°C$ (voltage coefficient).

\subsubsection{Model Validation}

Yang et al. (2021): < 2\% error vs experimental data, real-time computational efficiency, thermal dynamics included. Validation: 200--1000 W/m² irradiance, 15--45°C temperature range.

% ============================================================================
\section{Conventional Control Strategies}

\subsection{PI Controller Fundamentals}

PI controllers ($u(t) = K_p e(t) + K_i \int_0^t e(\tau) d\tau$, $G_c(s) = \frac{K_p s + K_i}{s}$) are widely deployed for simplicity, reliability, low computation (> 10 kHz), and zero steady-state error. Limitations: fixed parameters degrade performance away from design point, linear approximation breaks down under large signals/saturation/cross-coupling, single-objective focus requires separate controllers, and tuning complexity without optimality guarantee.

\subsection{Advanced Classical Control Methods}

**Backstepping:** Linares-Flores et al. (2015) achieved < 0.5\% tracking error with Lyapunov stability guarantees. Limitations: requires accurate model, complex implementation, computationally intensive.

**Sliding Mode Control:** Yao et al. (2016) implemented SMC ($s(x) = 0$, $u = u_{eq} + u_{sw}$) achieving 150 ms settling (vs 200 ms PI), 8\% overshoot (vs 12\% PI). Limitations: chattering (5-10\%), high switching frequency, complexity.

**Model Predictive Control:** Elbarbary et al. (2024) achieved 1.8\% THD, ±1.5\% DC link regulation via optimization:
\begin{equation}
\min_{u} \sum_{k=0}^{N_p} \left\| x_{ref}(k) - x(k) \right\|_Q^2 + \sum_{k=0}^{N_c} \left\| u(k) \right\|_R^2
\end{equation}

Recent MPC advances \cite{RealTimeMPC2025, DynamicMPC_PSO_2025}: 1 kHz update rates, 43\% cost reduction, PSO-enhanced tuning. Despite advances, MPC requires accurate models and expert tuning, motivating model-free RL approaches.



\textbf{Classical Control Comparison:} PI (low complexity/computation, no adaptability, low model dependency), Backstepping (high complexity, limited adaptability, medium computation, high model dependency), SMC (medium complexity/computation/dependency), MPC (very high complexity/computation/dependency, medium adaptability).

% ============================================================================
\section{Machine Learning in Power Systems}

\subsection{Evolution of AI in Power Systems}

\subsubsection{Early Applications (1990--2010)}

Early ML employed expert systems for fault diagnosis/load forecasting, artificial neural networks for load prediction/harmonic estimation, and fuzzy logic for MPPT/stabilizers/load frequency control.

\subsubsection{Modern Deep Learning Era (2010--Present)}

Wu et al. (2024) demonstrated DBN-based emergency control achieving 98.5\% accuracy, < 50 ms response, 1.2\% false alarms. Limitations for real-time control: no direct action output, requires separate control logic, unsuitable for continuous control, cannot learn optimal policies end-to-end through system interaction.

\subsection{Supervised Learning Approaches}

\subsubsection{Support Vector Machines}

SVMs: load forecasting, fault classification, power quality detection. Advantages: good generalization with limited data, high-dimensional handling, overfitting robustness. Control limitations: static mapping without temporal dynamics, no sequential decisions, requires expert features, cannot optimize long-term objectives.

\subsubsection{Deep Neural Networks}

DNNs: renewable forecasting, state estimation, cybersecurity. Benbouhenni et al. (2024) \cite{Benbouhenni2024} fractional-order fuzzy-NN for DFIG: THD 2.1\%, adaptive tuning, ±30\% robustness, superior transients.

\textbf{Hybrid Neuro-Fuzzy:} Combines fuzzy interpretability/expert knowledge with NN learning/approximation for human-understandable rules with data-driven optimization.

\textbf{Performance:} THD: PI 4.5\%, Fuzzy 3.2\%, Neuro-Fuzzy 2.1\%. Settling: baseline, -15\%, -35\%. Overshoot: 8\%, 5.5\%, 3.2\%. Adaptability: none, limited, high.

\subsection{Reinforcement Learning Emergence}

\subsubsection{Why RL for Power System Control?}

Supervised learning limitations: no sequential decisions, requires ground truth (often unknown), no exploration, static policies. RL advantages: learns from interaction without labels, optimizes long-term objectives, explores to discover novel strategies, adapts online, handles delayed consequences via credit assignment.

\subsubsection{Transition to Deep RL}

Traditional RL (Q-learning, SARSA) limitations: discrete actions only (cannot control continuous voltage/current), tabular representation (no scaling), cannot handle high-dimensional states, poor generalization. Deep RL solutions: NN function approximation for compact representation, continuous action spaces via actor-critic, generalization through learned features, effective scaling to complex high-dimensional systems.

The evolution of deep RL proceeded through several key milestones: Deep Q-Networks (DQN, 2013) first demonstrated that neural networks could approximate Q-functions for high-dimensional state spaces; Asynchronous Advantage Actor-Critic (A3C, 2016) introduced parallel actor-critic training for improved stability and efficiency; DDPG \cite{Lillicrap2015} then combined deterministic policy gradients with DQN's experience replay and target networks to enable continuous control; and TD3 \cite{Fujimoto2018} further refined actor-critic methods by addressing DDPG's overestimation bias. This progression from discrete to continuous, and from single-critic to twin-critic architectures, directly underpins the algorithms evaluated in this thesis.

% ============================================================================
\section{Deep Reinforcement Learning Fundamentals}

\subsection{Decision Process Formulation}

\subsubsection{Mathematical Framework}

Power system control as : $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$

\textbf{State Space} $\mathcal{S}$:
\begin{equation}
s_t = [i_{qs}, i_{ds}, i_{qr}, i_{dr}, v_{dc}, i_{pv}, P_s, Q_s, P_g, Q_g, \theta_r]^T \in \mathbb{R}^{11}
\end{equation}

\textbf{Action Space} $\mathcal{A}$:
\begin{equation}
a_t = [v_{qr}, v_{dr}, v_{qg}, v_{dg}]^T \in \mathbb{R}^4
\end{equation}

\textbf{State Transition} $\mathcal{P}$:
\begin{equation}
s_{t+1} = f(s_t, a_t) + w_t
\end{equation}
where $w_t$ represents process noise and uncertainties.

\textbf{Reward Function} $\mathcal{R}$:
\begin{equation}
r_t = \mathcal{R}(s_t, a_t) = r_{RSC} + r_{GSC}
\end{equation}

\textbf{Discount Factor} $\gamma \in [0, 1]$: Balances immediate vs. future rewards

\subsubsection{Control Objective}

Find optimal policy $\pi^*$ that maximizes expected cumulative reward:
\begin{equation}
\pi^* = \arg\max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]
\end{equation}

\subsection{Value Functions}

\subsubsection{State Value Function}

Expected return starting from state $s$ following policy $\pi$:
\begin{equation}
V^{\pi}(s) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s \right]
\end{equation}

\subsubsection{Action Value Function (Q-Function)}

Expected return from state-action pair:
\begin{equation}
Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s, a_0 = a \right]
\end{equation}

\textbf{Bellman Optimality Equation:}
\begin{equation}
Q^*(s, a) = \mathbb{E}_{s'} \left[ r(s, a) + \gamma \max_{a'} Q^*(s', a') \right]
\end{equation}

\subsection{Policy Gradient Methods}

\subsubsection{Stochastic vs. Deterministic Policies}

\textbf{Stochastic Policy:} $\pi(a|s)$ - probability distribution over actions
\begin{equation}
a_t \sim \pi(\cdot | s_t)
\end{equation}

\textbf{Deterministic Policy:} $\mu(s)$ - direct mapping to action
\begin{equation}
a_t = \mu(s_t)
\end{equation}

For continuous control (voltage references), deterministic policies are more suitable.

\subsubsection{Policy Gradient Theorem}

\textbf{Objective:}
\begin{equation}
J(\theta) = \mathbb{E}_{s \sim \rho^{\pi}} [V^{\pi}(s)]
\end{equation}

\textbf{Gradient:}
\begin{equation}
\nabla_{\theta} J(\theta) = \mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi} [\nabla_{\theta} \log \pi_{\theta}(a|s) Q^{\pi}(s, a)]
\end{equation}

\textbf{Deterministic Policy Gradient:}
\begin{equation}
\nabla_{\theta} J(\theta) = \mathbb{E}_{s \sim \rho^{\mu}} [\nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^{\mu}(s, a) |_{a = \mu_{\theta}(s)}]
\end{equation}

This is the foundation for DDPG and TD3 algorithms.

\subsection{Actor-Critic Architecture}

\subsubsection{Dual Network Structure}

Actor learns policy $\mu_{\theta}(s)$: input $s$, output $a = \mu_{\theta}(s)$, optimized to maximize Q-value. Critic learns Q-function $Q_{\phi}(s, a)$: input $(s, a)$, output Q-value, optimized via TD learning for accurate long-term reward prediction.

\subsubsection{Training Procedure}

\textbf{Critic Update (TD Learning):}
\begin{align}
y_t &= r_t + \gamma Q_{\phi'}(s_{t+1}, \mu_{\theta'}(s_{t+1})) \\
L(\phi) &= \mathbb{E}_{(s, a, r, s') \sim \mathcal{B}} [(y_t - Q_{\phi}(s_t, a_t))^2]
\end{align}

\textbf{Actor Update (Policy Gradient):}
\begin{equation}
\nabla_{\theta} J \approx \mathbb{E}_{s \sim \mathcal{B}} [\nabla_a Q_{\phi}(s, a) |_{a = \mu_{\theta}(s)} \nabla_{\theta} \mu_{\theta}(s)]
\end{equation}

% ============================================================================
\section{Deep Deterministic Policy Gradient (DDPG)}

\subsection{DDPG Fundamentals}

\subsubsection{Algorithm Overview}

Lillicrap et al. (2015) \cite{Lillicrap2015} introduced DDPG as a model-free, off-policy algorithm combining DPG, DQN (experience replay, target networks), and actor-critic methods. Key innovation: extends actor-critic to continuous action spaces via deterministic policy gradients, suitable for power system control.


\subsubsection{Core Components}
\begin{enumerate}

\item \textbf{Experience Replay Buffer:}
\begin{equation}
\mathcal{B} = \{(s_t, a_t, r_t, s_{t+1})\}_{t=1}^N
\end{equation}

Benefits: breaks temporal correlations (improves sample efficiency), enables off-policy learning, allows mini-batch training (stable gradients), improves data efficiency via reuse.

\item \textbf{Target Networks:}
\begin{align}
\theta' &= \tau \theta + (1 - \tau) \theta' \quad \text{(actor)} \\
\phi' &= \tau \phi + (1 - \tau) \phi' \quad \text{(critic)}
\end{align}

$\tau \ll 1$ (typically 0.001) ensures slow updates, stabilizing learning with consistent target values.

\item \textbf{Exploration Noise:}
\begin{equation}
a_t = \mu_{\theta}(s_t) + \mathcal{N}_t
\end{equation}

where $\mathcal{N}_t$ is typically Ornstein-Uhlenbeck noise:
\begin{equation}
d\mathcal{N}_t = \theta (\mu - \mathcal{N}_t) dt + \sigma dW_t
\end{equation}
    
\end{enumerate}
\subsection{DDPG in Power Systems}

\subsubsection{Wind Turbine Control}

Zhang et al. (2021) applied DDPG to DFIG wind turbine control:

\textbf{State Space Design:}
\begin{equation}
s = [i_{dr}, i_{qr}, i_{ds}, i_{qs}, \omega_r, v_{wind}, V_{dc}]^T
\end{equation}

\textbf{Action Space:}
\begin{equation}
a = [V_{dr}, V_{qr}]^T
\end{equation}

\textbf{Reward Function:}
\begin{equation}
r = -(w_1 (\omega_r - \omega_r^*)^2 + w_2 (P_s - P_s^*)^2 + w_3 (Q_s - Q_s^*)^2)
\end{equation}

Results: MPPT efficiency 97.8\%, settling 120 ms (vs 180 ms PI), overshoot 3.5\% (vs 6.2\% PI), training 1500 episodes (≈6 hours).

\subsubsection{Voltage Control in Distribution Systems}

Yang et al. (2020) demonstrated two-time-scale voltage control: fast (reactive power), slow (tap changer). Performance: voltage ±2\% (vs ±5\% IEEE standard), losses reduced 8.5\%, adaptive PV variability handling.

\subsubsection{Hybrid System Energy Management}

Chen et al. (2021) applied DDPG to PV/battery microgrid management:

\textbf{Control Objectives:}
\begin{enumerate}
    \item Minimize operating cost
    \item Maximize PV utilization
    \item Maintain battery State of Charge (SOC)
    \item Ensure power balance
\end{enumerate}

Results: cost reduced 12\% vs rule-based, battery life extended 15\% via optimized charging/discharging, grid power variability reduced 40\%.

\subsubsection{Grid Stability and Voltage Control}

DDPG has emerged as a powerful tool for grid-connected renewable energy systems, particularly for addressing stability challenges and power optimization in hybrid solar PV-integrated DFIG systems \cite{Pandey2025DDPG}:

\textbf{Rotor Angle Stability Enhancement:} DDPG-based power system stabilizers \cite{Yakout2025} demonstrate superior performance over multi-band PSS and conventional lead PSS controllers for transient stability improvement. Validation on single-machine infinite-bus (SMIB), Kundur 4-machine, and IEEE 39-bus systems confirms DDPG's effectiveness for maintaining synchronous stability in grids with high renewable penetration—critical for DFIG wind systems during grid disturbances.

\textbf{Adaptive Voltage Regulation:} DDPG-based synergetic control for synchronous generator excitation systems \cite{Fathollahi2024} achieves fast adaptive voltage regulation and stability improvement under various disturbances. This approach is highly relevant for hybrid DFIG-PV systems that must maintain grid voltage within IEEE 1547 limits (±5\%) during rapid solar irradiance and wind speed variations.

\subsection{DDPG Limitations}

\subsubsection{Overestimation Bias}

Critic errors lead to overestimated Q-values ($\mathbb{E}[Q_{\phi}(s, a)] > Q^*(s, a)$). Actor exploits errors causing: aggressive control beyond safe limits, policy instability with oscillations, system overshoots, poor generalization, training divergence in later stages.

Khalid et al. (2022) load frequency control: DDPG overshoot 11.2\% (above limits), oscillation 2-3s (instability), settling 14s (too slow), divergence after 1000+ episodes.

\subsubsection{High Variance in Learning}

Coupled actor-critic updates: unstable policies (critic changes affect actor), high Q-value variance (moving target), excessive hyperparameter sensitivity, inconsistent convergence across seeds.

\subsubsection{Hyperparameter Sensitivity}

DDPG requires careful tuning: learning rates ($\alpha_{\mu}$, $\alpha_Q$), target update rate ($\tau$), noise parameters ($\sigma$, $\theta$), network architecture (layers, neurons), training parameters (batch size, buffer size). Small changes (factor 2-3) can mean failure vs success.

% ============================================================================
\section{Twin Delayed Deep Deterministic Policy Gradient}

\subsection{TD3 Algorithm Development}

\subsubsection{Motivation and Origins}

Fujimoto et al. (2018) \cite{Fujimoto2018} introduced TD3 to address function approximation errors in actor-critic methods, building upon DDPG with three critical improvements.

\subsection{Three Key Innovations}

\subsubsection{Innovation 1: Clipped Double Q-Learning}

\textbf{Problem in DDPG:}
Single critic $Q_{\phi}$ tends to overestimate:
\begin{equation}
\mathbb{E}[Q_{\phi}(s, \mu_{\theta}(s))] \geq Q^{\mu}(s, \mu_{\theta}(s))
\end{equation}

\textbf{TD3 Solution:}
Use TWO independent critics $Q_{\phi_1}$ and $Q_{\phi_2}$:
\begin{equation}
y = r + \gamma \min_{i=1,2} Q_{\phi_i'}(s', \tilde{a}')
\end{equation}

where $\tilde{a}' = \mu_{\theta'}(s') + \epsilon$, $\epsilon \sim \text{clip}(\mathcal{N}(0, \sigma), -c, c)$

Rationale: minimum provides lower bound (conservative vs optimistic), reduces overestimation. Both critics trained independently (different initializations, sampling), minimum provides robustness to individual errors.

\textbf{Mathematical Justification:}
If critics have independent errors $\epsilon_1$ and $\epsilon_2$:
\begin{align}
Q_{\phi_1}(s, a) &= Q^*(s, a) + \epsilon_1 \\
Q_{\phi_2}(s, a) &= Q^*(s, a) + \epsilon_2
\end{align}

Then:
\begin{equation}
\mathbb{E}[\min(Q_{\phi_1}, Q_{\phi_2})] \approx Q^*(s, a) + \mathbb{E}[\min(\epsilon_1, \epsilon_2)] < Q^*(s, a) + \mathbb{E}[\epsilon_i]
\end{equation}

\subsubsection{Innovation 2: Delayed Policy Updates}

DDPG problem: updating actor every step causes policy to chase moving Q-target, resulting in high variance and instability.

\textbf{TD3 Solution:}
Update actor (and targets) every $d$ critic updates (typically $d = 2$):

\begin{algorithm}[H]
\caption{TD3 Update Schedule}
\begin{algorithmic}
\FOR{each training iteration $t$}
    \STATE Update both critics $Q_{\phi_1}$ and $Q_{\phi_2}$
    \IF{$t \mod d = 0$}
        \STATE Update actor $\mu_{\theta}$
        \STATE Update target networks $\theta'$, $\phi_1'$, $\phi_2'$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

Benefits: critics converge to better estimates before policy update, reduces actor-critic coupling, lowers variance, improves stability.

\subsubsection{Innovation 3: Target Policy Smoothing}

Problem: deterministic policies exploit narrow Q-function peaks (approximation errors), overfitting to spurious values, resulting in brittle policies with poor generalization.

\textbf{TD3 Solution:}
Add clipped noise to target action:
\begin{equation}
\tilde{a}' = \text{clip}(\mu_{\theta'}(s') + \text{clip}(\epsilon, -c, c), a_{low}, a_{high})
\end{equation}

where $\epsilon \sim \mathcal{N}(0, \sigma)$, typically $\sigma = 0.2$, $c = 0.5$

Effect: smooths value estimates around target action (not single point), regularizes similar to supervised learning, improves robustness to Q-approximation errors, prevents narrow peak exploitation.



\subsection{TD3 in Power System Applications}

\subsubsection{DFIG Wind Turbine Control}

Zholtayev et al. (2024) applied TD3 for virtual inertia and damping control in DFIG systems:

\textbf{Control Objectives:}
\begin{enumerate}
    \item Provide virtual inertia to grid
    \item Dampen power oscillations
    \item Maintain MPPT operation
    \item Ensure stability during transients
\end{enumerate}

\textbf{Physics-Constrained TD3:} Physical limits in reward function, constraint satisfaction via clipping/projection, model-based initialization for warm start.

\textbf{Results:} Frequency nadir +40\% vs conventional, RoCoF reduced 35\%, oscillation damping +50\% vs DDPG, robust to 30\% parameter variations, converged in 2000 episodes (≈10 hours).


\subsubsection{Load Frequency Control}

Khalid et al. (2022) TD3 vs DDPG for LFC: two-area system, 40\% wind/solar, random loads.

\textbf{Performance:} Overshoot/undershoot/settling: GA-PID (0.9\%/-0.9\%/18.5s), PSO-PID (1.0\%/-1.0\%/16.2s), DDPG (0.3\%/-1.1\%/14.1s), TD3 (0.15\%/-0.6\%/7.5s). TD3: 50\% lower overshoot, 47\% faster settling vs DDPG, more consistent, no oscillations.

\subsubsection{Converter Control}

Muktiadji et al. (2024) TD3 for DC-DC boost converter: PI gain optimization, voltage regulation ±0.5\% (vs ±2\% PSO-PI), rise time 8 ms (vs 15 ms PI), zero overshoot (vs 5\% PSO-PI), robust to 20\% voltage variations.

\subsubsection{Microgrid Stability}

Lee et al. (2023) TD3 dynamic droop control for AC microgrids: adaptive coefficients, frequency < 0.1 Hz (vs 0.3 Hz fixed), voltage ±1.5\% (vs ±3\% fixed), power sharing < 2\% error.

\subsubsection{Recent TD3 Advances (2024--2025)}

\textbf{Virtual Power Plant Coordination \cite{VPP_TD3_2025}:} TD3-based DER coordination: wind/solar/storage aggregation, multi-objective optimization, 16.7\% cost reduction and 84.71\% higher mean reward vs DDPG baselines, scales to 100+ units.

\textbf{Multi-Agent TD3 for AGC \cite{MATD3_AGC_2025}:} Decentralized multi-area control: independent agents per area, cooperative learning via shared critics, frequency < 0.05 Hz, 40\% faster response vs centralized.

\textbf{Optimal Voltage-Frequency Control \cite{OptimalVoltageFrequency2025}:} Coordinated control for renewable grids: UPFC + EV integration, 60--80\% RES penetration, handles variability and uncertainty.

\textbf{Emerging Trends in TD3 Applications:}
\begin{enumerate}
    \item \textbf{Multi-Agent Extensions:} Scalable coordination of distributed resources
    \item \textbf{Hybrid Architectures:} Combining TD3 with MPC for constraint satisfaction
    \item \textbf{Transfer Learning:} Pre-trained policies adapted to new systems
    \item \textbf{Safety Enhancements:} Constrained TD3 with formal guarantees
    \item \textbf{Real-Time Deployment:} FPGA and edge computing implementations
\end{enumerate}

\subsubsection{TD3 Actor-Critic Architecture}

The TD3 architecture with twin critic networks is illustrated in Figure~\ref{fig:td3_actor_critic}, which shows how the dual critic approach mitigates overestimation bias by taking the minimum Q-value estimate for policy updates.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{images/TD3ActorCritic.png}
    \caption{TD3 actor-critic network architecture with twin critic networks ($Q_1$ and $Q_2$) for improved value estimation and reduced overestimation bias in DFIG-Solar PV hybrid system control}
    \label{fig:td3_actor_critic}
\end{figure}

\subsection{Comparative Analysis: DDPG vs. TD3}

Both algorithms share the deterministic policy gradient foundation, but TD3's three innovations directly address DDPG's documented weaknesses. Twin critics reduce Q-value overestimation bias---Khalid et al. (2022) \cite{Khalid2022} demonstrated 50\% overshoot reduction and 47\% settling time improvement when replacing DDPG with TD3 for load frequency control. Delayed policy updates prevent rapid policy changes based on noisy Q-estimates, reducing policy oscillation during training. Target policy smoothing improves generalization and reduces sensitivity to hyperparameter choices, enhancing overall robustness to parameter variations. However, DDPG remains computationally simpler during training (single critic, no delay scheduling) and may be preferable for real-time adaptive control scenarios where training overhead is critical. A systematic review of RL-based microgrid control \cite{Waghmare2025} confirms that while TD3 generally outperforms DDPG in stability-critical applications, the choice depends on specific deployment constraints including computational budget and real-time requirements.

\subsection{Beyond TD3: Advanced Actor-Critic Methods}

\subsubsection{Soft Actor-Critic (SAC)}

While DDPG and TD3 use deterministic policies, Soft Actor-Critic (SAC) introduces entropy regularization for enhanced exploration and robustness.

\textbf{Key Innovation - Maximum Entropy RL:}
\begin{equation}
J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(s_t, a_t) \sim \rho_{\pi}} [r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot | s_t))]
\end{equation}

where $\mathcal{H}(\pi(\cdot | s_t)) = -\mathbb{E}_{a \sim \pi}[\log \pi(a|s_t)]$ is the entropy, encouraging exploration.

\textbf{SAC Architecture:} Stochastic policy $a_t \sim \pi_{\theta}(\cdot | s_t)$ (Gaussian), twin Q-networks (like TD3), automatic entropy tuning ($\alpha$), off-policy with replay.

\textbf{VSG Control with SAC \cite{SAC_VSG_2024}:} Virtual synchronous generators: frequency < 0.08 Hz, 30\% better transients vs TD3, robust to noise.

\textbf{Multi-Agent SAC for Voltage \cite{MASAC_Voltage_2024}:} Distribution voltage regulation: decentralized execution/centralized training, voltage < 1\%, handles 50\% PV.

\textbf{SAC vs TD3:} Policy (deterministic/stochastic), exploration (noise/entropy), noise robustness (good/excellent), sample efficiency (high/moderate), stability (very high/high), tuning (easier/complex).

\textbf{Microgrid Energy Management:} Improved SAC algorithms \cite{Yu2024} achieve 51.20\%, 52.38\%, and 13.43\% profit increases over DQN, SARSA, and PPO respectively for microgrid optimization. SAC's application to integrated energy systems with electricity, heat, and hydrogen storage \cite{Liang2024} demonstrates model-free control for multi-energy systems with complex energy coupling---relevant for future DFIG-PV systems integrated with storage.

\textbf{When to Use SAC:} High noise, stochastic dynamics, exploration-heavy, multi-modal policies.

\textbf{When TD3 is Preferred:} Deterministic dynamics, sample efficiency critical, faster convergence, tight computational constraints.

\subsubsection{Other Actor-Critic Variants}

\textbf{Proximal Policy Optimization (PPO):}
On-policy method with clipped objective, widely used but lower sample efficiency than off-policy methods for power systems.

\textbf{Multi-Agent Deep Deterministic Policy Gradient (MADDPG):}
Extension of DDPG for multi-agent settings, suitable for coordinated control of multiple generators.

\textbf{Distributed Distributional DDPG (D4PG):}
Combines distributional RL with DDPG, learning value distribution rather than expected value.

\subsubsection{Multi-Objective Deep RL for Wind Turbines}

Actor-critic deep RL enables multi-objective optimization beyond simple MPPT \cite{Frutos2025}. For 2.3 MW wind turbines, deep RL balances power maximization with operational constraints such as noise mitigation, mechanical stress reduction, and grid code compliance. This multi-objective capability is essential for commercial DFIG installations where regulatory compliance, equipment lifespan, and power quality must be simultaneously optimized.

\subsubsection{Quantum-Inspired Actor-Critic Methods}

Emerging quantum-inspired approaches \cite{Shen2024} demonstrate improved performance for frequency regulation in renewable-dominated microgrids. Quantum-inspired maximum entropy actor-critic (QISMEAC) algorithms outperform standard SAC and DDPG for handling renewable intermittency, offering potential pathways for next-generation control systems capable of managing high-penetration renewable grids with minimal frequency deviations (±0.2 Hz IEEE 1547 requirement).

\subsection{Safety-Constrained Reinforcement Learning}

A critical challenge for deploying DRL controllers in power systems is ensuring safety during both training (exploration) and deployment. Standard DRL algorithms including DDPG and TD3 rely on unconstrained exploration that may produce actions violating operational limits---potentially causing equipment damage or grid instability. Recent reviews \cite{Waghmare2025, Li2024} identify safe RL as an emerging research priority for power system applications. Approaches include constrained Markov decision processes (CMDPs) with Lagrangian relaxation to enforce voltage and frequency limits, barrier function-based reward shaping that penalizes proximity to constraint boundaries, and physics-constrained architectures such as the physics-constrained TD3 demonstrated by Zholtayev et al. (2024) \cite{PhysicsConstrainedTD32024} for DFIG virtual inertia control, which integrates quadratic programming constraints directly into the TD3 framework. Despite these advances, formal safety guarantees for DRL controllers in grid-connected renewable systems remain an open problem, particularly for the tightly coupled dynamics of DFIG-PV hybrid systems.

\subsection{Simulation-to-Real Transfer Challenges}

While DRL controllers achieve strong performance in simulation environments, bridging the gap to real-world deployment remains a significant challenge \cite{Ejiyi2025}. Key issues include: (i) model fidelity---simulation models may not capture all parasitic effects, switching transients, and sensor noise present in physical hardware; (ii) computational latency---neural network inference must meet real-time constraints (typically < 100 $\mu$s for power electronic control loops); and (iii) distribution shift---operating conditions encountered in deployment may differ from training distributions. Hardware-in-the-loop (HIL) testing platforms such as OPAL-RT provide an intermediate validation step between pure simulation and field deployment, enabling real-time execution with realistic signal conditioning and communication delays \cite{Pandey2022PIICON}. However, systematic methodologies for sim-to-real transfer of DRL controllers in power systems---including domain randomization, online adaptation, and robustness certification---remain largely unexplored.

\subsection{Summary of DRL Approaches for Power Systems}

Recent comprehensive reviews \cite{Li2024, Tang2024, Waghmare2025} identify several fundamental advantages of deep reinforcement learning for DFIG-PV hybrid systems. Unlike model predictive control (MPC) \cite{Joshal2023, Kamal2022}, DRL directly learns optimal policies from nonlinear system dynamics without requiring linearization or explicit system models. DRL controllers trained with diverse operating conditions demonstrate robust performance across parameter uncertainties inherent in renewable energy systems including turbine inertia variations, PV array degradation, and grid impedance changes \cite{Ejiyi2025}. Actor-critic methods naturally handle continuous control signals such as rotor-side converter voltages and grid-side converter modulation indices without discretization artifacts \cite{Lillicrap2015, Fujimoto2018}. Reward function design enables simultaneous optimization of multiple conflicting objectives including power maximization, grid stability, THD minimization, and fault ride-through capability \cite{Frutos2025}.

These advantages position TD3 and DDPG as promising alternatives to traditional PI/PID cascades and model-based controllers for next-generation hybrid renewable energy systems \cite{Sofian2024, AbdulMajeed2025}.

% ============================================================================
\section{Research Gaps and Opportunities}

\subsection{Identified Critical Gaps}



\subsubsection{Gap 1: Unified Control of Tightly Coupled Systems}

\textbf{Current:} Focus on single sources (wind or solar), limited DFIG-PV coupling investigation. Separate subsystem controllers with coordination layers (suboptimal). Unified frameworks rare.

\textbf{Challenge:} Strong coupling: PV→DC link→rotor converter→stator power; GSC balances both sources.

\textbf{Opportunity:} Unified RL observes all states, coordinates actions, optimizes overall performance, learns coupling implicitly.

\subsubsection{Gap 2: Comprehensive DRL Algorithm Comparison}

\textbf{Current:} Comparisons mostly vs PI baselines, limited DRL head-to-head under identical conditions. Inconsistent metrics/systems hinder fair comparisons.

\textbf{Missing:} DDPG vs TD3 on identical systems, overestimation bias quantification, training efficiency (sample/time), robustness under variations, computational cost (training/deployment).


\subsection{Research Opportunities Addressed in This Thesis}

\subsubsection{Opportunity 1: Unified Control Framework}

Single unified TD3 controller managing RSC, GSC, and PV simultaneously. Learns coupled dynamics end-to-end, eliminates coordination layer, optimizes overall performance. 11-D state: $i_{qs}$, $i_{ds}$, $i_{qr}$, $i_{dr}$, $v_{dc}$, $i_{pv}$, $P_s$, $Q_s$, $P_g$, $Q_g$, $\theta_r$.

\subsubsection{Opportunity 2: Comprehensive DDPG vs. TD3 Comparison}

Rigorous side-by-side comparison on identical DFIG-PV system (same reward, state, action spaces). Consistent metrics, training efficiency analysis, systematic robustness testing. Quantifies overestimation bias reduction, analyzes individual TD3 innovations, computational cost breakdown, deployment considerations.

\subsubsection{Opportunity 3: Hardware-in-Loop Validation}

OPAL-RT HIL validation: microsecond switching dynamics, realistic constraints, sensor noise/quantization/delays. Extends beyond simulation, demonstrates real-time feasibility, identifies implementation challenges, provides deployment confidence.



% 

% ============================================================================
% END OF CHAPTER 2
% ============================================================================