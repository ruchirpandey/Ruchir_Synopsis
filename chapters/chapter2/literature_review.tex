% ============================================================================
% CHAPTER 2: COMPREHENSIVE LITERATURE REVIEW (EXPANDED)
% PhD Thesis: DFIG-Solar PV Interconnected Hybrid Energy Generation System
% ============================================================================

\section{Introduction to Literature Review}

This chapter reviews literature on DFIG-Solar PV hybrid systems and control strategies, tracing evolution from conventional to deep reinforcement learning approaches, emphasizing DDPG and TD3 applications in power systems. The review is based upon recent comprehensive surveys on AI in renewable power systems \cite{Li2024, Tang2024} and RL-based microgrid control \cite{Waghmare2025}.

% ============================================================================
\section{Wind-Solar Hybrid Systems}

Hybrid wind-solar integration has evolved from simple parallel operation to sophisticated coordinated control over the past two decades. The paradigm has shifted from separate optimized control to integrated controls as discussed in detail in further subsection.

\subsection{Early Hybrid Systems (2000--2010)}
Early hybrid systems had independent converters, separate MPPT algorithms, minimal coordination (Lasseter et al. CERTS microgrid). High costs and suboptimal efficiency limited deployment. Since then Power electronics and computational technologies have evolved and matured , now it is possible to have both integrated systems and control.

\subsection{Integrated Architectures (2010--2020)}

DC link integration of solar PV at DFIG converters (Tiwari et al., 2018) achieved 15--20\% converter reduction and 8--12\% efficiency improvement through unified voltage regulation and coordinated MPPT. Kumar et al. (2020) demonstrated dual MPPT achieving 8--15\% annual energy improvement. This architecture eliminates separate converters while maintaining independent power tracking, optimally balancing simplification and control flexibility.

\subsubsection{Battery Energy Storage Integration}

Bhattacharyya and Singh (2022) integrated BESS with DFIG-PV systems, achieving 45\% power fluctuation reduction, 25\% improved fault ride-through, and DSOSF-FLL control with < 20 ms settling. Recent work \cite{WindBattery2025} demonstrates lithium-ion integration achieving 60-75\% output variability reduction. IRENA \cite{IRENA_BESS_2025} reports 40\% cost decline since 2020, > 90\% round-trip efficiency, and 15-20 year lifespan. BESS provides power quality support, energy arbitrage, primary frequency response, and backup capability.

\subsubsection{Grid Stability and Frequency Regulation (2024--2025)}

Smahi et al. (2025) \cite{Smahi2025} review grid inertia solutions including VSM for synthetic inertia, fast-responding BESS, and AI-driven adaptive control. He et al. (2024) \cite{He2024} achieve 200 ms wind turbine FFR, 45\% better frequency, and 35\% RoCoF reduction. MSESO-GADRC \cite{MSESO_GADRC_2025} demonstrates 60\% disturbance rejection improvement over PI with robustness to 40\% parameter variations.

\textbf{Stability Parameters:}
\begin{itemize}
    \item Frequency deviation: ±0.2 Hz standard (±0.05--0.10 Hz advanced)
    \item RoCoF: 1.0 Hz/s standard (0.3--0.5 Hz/s advanced)
    \item Voltage: ±5\% standard (±1--2\% advanced)
    \item FFR response: < 500 ms standard (150--250 ms advanced)
\end{itemize}

\subsection{Modern Smart Grid Integration (2020--Present)}

Recent developments include energy internet concepts (Yue and Han, 2019), multi-microgrid architectures (Hong et al., 2018), and prosumer-based systems where consumer becomes producers with renewable energy (Zafar et al., 2018). Prasad et al. (2025) achieved 98.5\% MPPT efficiency with coordinated RSC-GSC control. GridIntegratedDFIG2025 demonstrated adaptive control achieving 2.60\% THD via OPAL-RT validation. Hybrid MPPT \cite{HybridMPPT2024} achieves 97.8--98.9\% efficiency versus 80--88\% conventional. Modern systems feature enhanced grid support, predictive control, ML-based optimization, and storage integration \cite{WindBattery2025}.

% ============================================================================
\section{Doubly Fed Induction Generator Technology}

\subsection{DFIG Fundamentals}

DFIG dominates variable-speed wind conversion with ~65\% market share (2024). The stator connects directly to the grid while the rotor uses back-to-back converters rated at 25--30\% of generator capacity, reducing cost versus full-scale converters in PMSG systems.

\subsubsection{Power Flow Equations}

\textbf{Stator Power:}
\begin{align}
P_s &= \frac{3}{2} V_s \left(\frac{L_m}{L_s} i_{dr} \sin\delta - i_{qr} \cos\delta \right) \\
Q_s &= \frac{3}{2} V_s \left(\frac{V_s}{\omega_s L_s} - \frac{L_m}{L_s} i_{dr} \cos\delta - i_{qr} \sin\delta \right)
\end{align}

\textbf{Rotor Power:} $P_r = s \cdot P_s$ where $s = \frac{\omega_s - \omega_r}{\omega_s}$

\subsection{d-q Reference Frame Modeling}

\textbf{Voltage Equations:}
\begin{align}
V_{qs} &= R_s i_{qs} + \frac{d\psi_{qs}}{dt} + \omega_s \psi_{ds}, \quad
V_{ds} = R_s i_{ds} + \frac{d\psi_{ds}}{dt} - \omega_s \psi_{qs} \\
V_{qr} &= R_r i_{qr} + \frac{d\psi_{qr}}{dt} + (\omega_s - \omega_r) \psi_{dr}, \quad
V_{dr} = R_r i_{dr} + \frac{d\psi_{dr}}{dt} - (\omega_s - \omega_r) \psi_{qr}
\end{align}

\textbf{Flux Linkages:}
\begin{align*}
\psi_{qs} = L_s i_{qs} + L_m i_{qr}, \quad \psi_{ds} = L_s i_{ds} + L_m i_{dr} \\
\psi_{qr} = L_r i_{qr} + L_m i_{qs}, \quad \psi_{dr} = L_r i_{dr} + L_m i_{ds}
\end{align*}

\subsection{Vector Control Principles}

Stator flux orientation ($\psi_{qs} = 0$) decouples active/reactive power control:
\begin{align}
P_s &\approx -\frac{3}{2} \frac{V_s L_m}{L_s} i_{qr} \\
Q_s &\approx \frac{3}{2} \frac{V_s}{L_s} \left( V_s - L_m i_{dr} \right)
\end{align}

Control objectives: MPPT, DC link regulation, grid synchronization, reactive power support, fault ride-through.

\subsection{Challenges in DFIG Control}

Wang et al. (2015) identified parameter variations, grid disturbances, mechanical dynamics, and sensor noise. Hu et al. (2019) addressed these via multi-agent systems achieving 15\% power quality improvement and 30\% stress reduction.

\subsection{Power Quality and Harmonic Mitigation}

MAO-RERNN \cite{MAO_RERNN_2025} achieves 1.56\% THD with 100 μs response time. Critical harmonics \cite{WindHarmonics2025} include 5th, 7th, 11th, and 13th orders.

\textbf{Mitigation Approaches:}
\begin{itemize}
    \item Passive filters: 3.5--5\% THD, low complexity
    \item Active PI: 2.5--3.5\% THD, 1--2 ms response
    \item Neuro-fuzzy: 2--2.5\% THD, 0.5--1 ms response
    \item MAO-RERNN: 1.5--1.8\% THD, < 0.1 ms response, very high complexity
\end{itemize}

\subsubsection{Power Quality Standards}

\textbf{IEEE 519-2014:} TDD < 5\%, harmonics 3rd/5th < 4\%, 7th < 2\%, 11th--17th < 1.5\%, voltage THD < 5\%.

\textbf{IEC 61000-3-6:} Voltage unbalance < 2\%, flicker Pst < 1.0 / Plt < 0.8.

\textbf{Quality Impacts:} Heating, resonance, equipment malfunction, power factor degradation.

% ============================================================================
\section{Solar PV Integration Architectures}

\subsection{PV System Modeling}
The single diode model was chosen for the modeling of solar-PV as it provides fast calculation of the parameters.

\subsection{Integration Topologies}

\begin{enumerate}
    \item Independent dedicated converters with simple control and  high cost/losses.
\item DC link integration with PV at DFIG DC link via boost and shared GSC resulting in 15--25\% cost reduction. 
\end{enumerate}
The challenges with integrated topology are coordination, stability and  MPPT interactions. Nindra et al. (2019) showed  ±2\% voltage, THD < 4\% for integrated topology. The preliminary real-time investigation of this architecture using OPAL-RT platform \cite{Pandey2022PIICON} confirmed feasibility of solar PV integration at the DFIG DC link with stable operation under varying wind and irradiance conditions.

\subsection{Maximum Power Point Tracking}

P\&O algorithm: perturbs voltage, observes power change, adjusts direction. Tracking: 95-98\%, simple but oscillates. Incremental Conductance ($\frac{dP}{dV} = 0 \Rightarrow \frac{dI}{dV} = -\frac{I}{V}$) achieves 97-99\% with less oscillation but higher complexity.

Advanced methods: Deep RL-based MPPT achieves 99.2\% efficiency with < 0.5s convergence and no steady-state oscillation \cite{MPPT_AI_Review_2025}. Kumar et al. (2024) unified controller achieves 98.5\% efficiency managing wind and solar simultaneously with reduced sensors.


% ============================================================================
\section{Conventional Control Strategies}

\subsection{PI Controller Fundamentals}

% PI controllers ($u(t) = K_p e(t) + K_i \int_0^t e(\tau) d\tau$, $G_c(s) = \frac{K_p s + K_i}{s}$)
PI controllers are widely deployed for simplicity, reliability, low computation > 10 kHz, and zero steady-state error. The limitations are the fixed parameters degrade performance away from design point, linear approximation breaks down under large signals/saturation/cross-coupling, single-objective focus requires separate controllers, and tuning complexity without optimality assurance.

\subsection{Advanced Classical Control Methods}
\\
\textbf{Sliding Mode Control:} Yao et al. (2016) implemented SMC ($s(x) = 0$, $u = u_{eq} + u_{sw}$) achieving 150 ms settling (vs 200 ms PI), 8\% overshoot (vs 12\% PI). Limitations: chattering (5-10\%), high switching frequency, complexity.

\textbf{Model Predictive Control:} Elbarbary et al. (2024) achieved 1.8\% THD, ±1.5\% DC link regulation via optimization:
\begin{equation}
\min_{u} \sum_{k=0}^{N_p} \left\| x_{ref}(k) - x(k) \right\|_Q^2 + \sum_{k=0}^{N_c} \left\| u(k) \right\|_R^2
\end{equation}

\textbf{Recent MPC advances} \cite{RealTimeMPC2025, DynamicMPC_PSO_2025}: 1 kHz update rates, 43\% cost reduction, PSO-enhanced tuning. Despite advances, MPC requires accurate models and expert tuning, motivating model-free RL approaches.



\textbf{Classical Control Comparison:} PI shows low complexity and computation, low adaptability and low model dependency, The Backstepping method has high complexity, limited adaptability, medium computation, and high model dependency, SMC shows medium complexity and computation dependency and MPC has very high complexity,computation, model dependency but shows better adaptability.

% ============================================================================
\section{Machine Learning(ML) in Power Systems}

\subsection{Evolution of ML in Power Systems}

\subsubsection{Early Applications of ML (1990--2010)}

Early ML models employed expert systems for fault diagnosis, load forecasting, artificial neural networks for load prediction,harmonic estimation and fuzzy logic for MPPT,stabilizers and load frequency control.

\subsubsection{Modern Deep Learning Era for ML (2010--Present)}

Wu et al. (2024) demonstrated DBN-based emergency control achieving 98.5\% accuracy, < 50 ms response, 1.2\% false alarms. Limitations for real-time control: no direct action output, it requires separate control logic and is unsuitable for continuous control. It cannot learn optimal policies end-to-end through system interaction.

\subsection{Supervised Learning Approaches}

\subsubsection{Support Vector Machines (SVM)}

SVMs are used in load forecasting, fault classification and power quality detection. The advantages of SVMs are good generalization with limited data, high-dimensional handling capability and certain amount of immunity for overfitting. The control limitations for SVMs are static mapping without temporal dynamics, no sequential decisions, requires expert features and it cannot optimize long-term objectives.

\subsubsection{Deep Neural Networks}

DNNs are used in renewable forecasting, state estimation, cybersecurity.

Benbouhenni et al. (2024) \cite{Benbouhenni2024} applied fractional-order fuzzy-NN for DFIG with THD 2.1\%, adaptive tuning, ±30\% robustness and superior transients.

\textbf{Hybrid Neuro-Fuzzy:} Combines fuzzy interpretability/expert knowledge with NN learning/approximation for human-understandable rules with data-driven optimization.

\textbf{Comparative Performance:} 
\begin{itemize}
    \item THD: PI 4.5\%, Fuzzy 3.2\%, Neuro-Fuzzy 2.1\%. 
    \item Settling: baseline, -15\%, -35\%. 
    \item Overshoot: 8\%, 5.5\%, 3.2\%. 
    \item Adaptability: none, limited, high respectively.
\end{itemize}

\subsection{Reinforcement Learning Emergence}

\subsubsection{RL for Power System Control}

The supervised learning had limitations like no sequential decisions, requires ground truth often unknown, no exploration and static policies. The RL has certain advantages as it learns from interaction without labels, optimizes long-term objectives, explores to discover novel strategies, adapts online, handles consequences via rewards.

\subsubsection{Transition to Deep RL}

Traditional RL (Q-learning, SARSA) has the limitations of having discrete actions only and cannot control continuous voltage and currents, tabular representation having no scaling, cannot handle high-dimensional states and poor generalization. The Deep RL gives the solutions by NN function approximation for compact representation, continuous action spaces via actor-critic, generalization through learned features and have effective scaling to complex high-dimensional systems.

The evolution of deep RL proceeded through several key milestones: 
\begin{itemize}
    \item Deep Q-Networks (DQN, 2013) first demonstrated that neural networks could approximate Q-functions for high-dimensional state spaces.
\item Asynchronous advantage Actor-Critic (A3C, 2016) introduced parallel actor-critic training for improved stability and efficiency.
\item DDPG \cite{Lillicrap2015} then combined deterministic policy gradients with DQN's experience replay and target networks to enable continuous control.
\item TD3 \cite{Fujimoto2018} further refined actor-critic methods by addressing DDPG's overestimation bias. This progression from discrete to continuous, and from single-critic to twin-critic architectures, directly underpins the algorithms evaluated in this thesis.
\end{itemize}

% ============================================================================
\section{Deep Reinforcement Learning}

\subsection{Formulation of Decision Process}

\subsubsection{Mathematical Framework}

Power system control as : $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$

\textbf{State Space} $\mathcal{S}$:
\begin{equation}
s_t = [i_{qs}, i_{ds}, i_{qr}, i_{dr}, v_{dc}, i_{pv}, P_s, Q_s, P_g, Q_g, \theta_r]^T \in \mathbb{R}^{11}
\end{equation}

\textbf{Action Space} $\mathcal{A}$:
\begin{equation}
a_t = [v_{qr}, v_{dr}, v_{qg}, v_{dg}]^T \in \mathbb{R}^4
\end{equation}

\textbf{State Transition} $\mathcal{P}$:
\begin{equation}
s_{t+1} = f(s_t, a_t) + w_t
\end{equation}
where $w_t$ represents process noise and uncertainties.

\textbf{Reward Function} $\mathcal{R}$:
\begin{equation}
r_t = \mathcal{R}(s_t, a_t) = r_{RSC} + r_{GSC}
\end{equation}

\textbf{Discount Factor} $\gamma \in [0, 1]$: Balances immediate vs. future rewards

\subsubsection{Control Objective}

Find optimal policy $\pi^*$ that maximizes expected cumulative reward:
\begin{equation}
\pi^* = \arg\max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]
\end{equation}

\subsection{Value Functions}

\subsubsection{State Value Function}

Expected return starting from state $s$ following policy $\pi$:
\begin{equation}
V^{\pi}(s) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s \right]
\end{equation}

\subsubsection{Action Value Function (Q-Function)}

Expected return from state-action pair:
\begin{equation}
Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s, a_0 = a \right]
\end{equation}

\textbf{Bellman Optimality Equation:}
\begin{equation}
Q^*(s, a) = \mathbb{E}_{s'} \left[ r(s, a) + \gamma \max_{a'} Q^*(s', a') \right]
\end{equation}

\subsection{Policy Gradient Methods}

\subsubsection{Stochastic vs. Deterministic Policies}

\textbf{Stochastic Policy:} $\pi(a|s)$ - probability distribution over actions
\begin{equation}
a_t \sim \pi(\cdot | s_t)
\end{equation}

\textbf{Deterministic Policy:} $\mu(s)$ - direct mapping to action
\begin{equation}
a_t = \mu(s_t)
\end{equation}

For continuous control (voltage references), deterministic policies are more suitable.

\subsubsection{Policy Gradient Theorem}

\textbf{Objective:}
\begin{equation}
J(\theta) = \mathbb{E}_{s \sim \rho^{\pi}} [V^{\pi}(s)]
\end{equation}

\textbf{Gradient:}
\begin{equation}
\nabla_{\theta} J(\theta) = \mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi} [\nabla_{\theta} \log \pi_{\theta}(a|s) Q^{\pi}(s, a)]
\end{equation}

\textbf{Deterministic Policy Gradient:}
\begin{equation}
\nabla_{\theta} J(\theta) = \mathbb{E}_{s \sim \rho^{\mu}} [\nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^{\mu}(s, a) |_{a = \mu_{\theta}(s)}]
\end{equation}

This is the foundation for DDPG and TD3 algorithms.

\subsection{Actor-Critic Architecture}

\subsubsection{Dual Network Structure}

Actor learns policy $\mu_{\theta}(s)$: input $s$, output $a = \mu_{\theta}(s)$, optimized to maximize Q-value. Critic learns Q-function $Q_{\phi}(s, a)$: input $(s, a)$, output Q-value, optimized via TD learning for accurate long-term reward prediction.

\subsubsection{Training Procedure}

\textbf{Critic Update (TD Learning):}
\begin{align}
y_t &= r_t + \gamma Q_{\phi'}(s_{t+1}, \mu_{\theta'}(s_{t+1})) \\
L(\phi) &= \mathbb{E}_{(s, a, r, s') \sim \mathcal{B}} [(y_t - Q_{\phi}(s_t, a_t))^2]
\end{align}

\textbf{Actor Update (Policy Gradient):}
\begin{equation}
\nabla_{\theta} J \approx \mathbb{E}_{s \sim \mathcal{B}} [\nabla_a Q_{\phi}(s, a) |_{a = \mu_{\theta}(s)} \nabla_{\theta} \mu_{\theta}(s)]
\end{equation}

% ============================================================================
\section{Deep Deterministic Policy Gradient (DDPG)}

\subsection{DDPG Fundamentals}

\subsubsection{Algorithm Overview}

Lillicrap et al. (2015) \cite{Lillicrap2015} introduced DDPG as a model-free, off-policy algorithm combining DPG, DQN (experience replay, target networks), and actor-critic methods. It extends actor-critic to continuous action spaces via deterministic policy gradients, suitable for power system control.


\subsubsection{Core Components}
\begin{enumerate}

\item \textbf{Experience Replay Buffer:}
\begin{equation}
\mathcal{B} = \{(s_t, a_t, r_t, s_{t+1})\}_{t=1}^N
\end{equation}

It improves sample efficiency, enables off-policy learning, allows mini-batch training with stable gradients and improves data efficiency via reuse.

\item \textbf{Target Networks:}
\begin{align}
\theta' &= \tau \theta + (1 - \tau) \theta' \quad \text{(actor)} \\
\phi' &= \tau \phi + (1 - \tau) \phi' \quad \text{(critic)}
\end{align}

$\tau \ll 1$ (typically 0.001) ensures slow updates, stabilizing learning with consistent target values.

\item \textbf{Exploration Noise:}
\begin{equation}
a_t = \mu_{\theta}(s_t) + \mathcal{N}_t
\end{equation}

where $\mathcal{N}_t$ is typically Ornstein-Uhlenbeck noise:
\begin{equation}
d\mathcal{N}_t = \theta (\mu - \mathcal{N}_t) dt + \sigma dW_t
\end{equation}
    
\end{enumerate}
\subsection{DDPG in Power Systems}

\subsubsection{Wind Turbine Control}

Zhang et al. (2021) applied DDPG to DFIG wind turbine control:

\textbf{State Space Design:}
\begin{equation}
s = [i_{dr}, i_{qr}, i_{ds}, i_{qs}, \omega_r, v_{wind}, V_{dc}]^T
\end{equation}

\textbf{Action Space:}
\begin{equation}
a = [V_{dr}, V_{qr}]^T
\end{equation}

\textbf{Reward Function:}
\begin{equation}
r = -(w_1 (\omega_r - \omega_r^*)^2 + w_2 (P_s - P_s^*)^2 + w_3 (Q_s - Q_s^*)^2)
\end{equation}

Results: MPPT efficiency 97.8\%, settling 120 ms (vs 180 ms PI), overshoot 3.5\% (vs 6.2\% PI), training 1500 episodes (≈6 hours).

\subsubsection{Voltage Control in Distribution Systems}

Yang et al. (2020) demonstrated two-time-scale voltage control: fast (reactive power), slow (tap changer). Performance: voltage ±2\% (vs ±5\% IEEE standard), losses reduced 8.5\%, adaptive PV variability handling.

\subsubsection{Hybrid System Energy Management}

Chen et al. (2021) applied DDPG to PV/battery microgrid management:

\textbf{Control Objectives:}
\begin{enumerate}
    \item Minimize operating cost
    \item Maximize PV utilization
    \item Maintain battery State of Charge (SOC)
    \item Ensure power balance
\end{enumerate}

Results: cost reduced 12\% vs rule-based, battery life extended 15\% via optimized charging/discharging, grid power variability reduced 40\%.

\subsubsection{Grid Stability and Voltage Control}

DDPG has emerged as a powerful tool for grid-connected renewable energy systems, particularly for addressing stability challenges and power optimization in hybrid solar PV-integrated DFIG systems \cite{Pandey2025DDPG}:

\textbf{Rotor Angle Stability Enhancement:} DDPG-based power system stabilizers \cite{Yakout2025} demonstrate superior performance over multi-band PSS and conventional lead PSS controllers for transient stability improvement. Validation on single-machine infinite-bus (SMIB), Kundur 4-machine, and IEEE 39-bus systems confirms DDPG's effectiveness for maintaining synchronous stability in grids with high renewable penetration—critical for DFIG wind systems during grid disturbances.

\textbf{Adaptive Voltage Regulation:} DDPG-based synergetic control for synchronous generator excitation systems \cite{Fathollahi2024} achieves fast adaptive voltage regulation and stability improvement under various disturbances. This approach is highly relevant for hybrid DFIG-PV systems that must maintain grid voltage within IEEE 1547 limits (±5\%) during rapid solar irradiance and wind speed variations.

\subsection{DDPG Limitations}

\subsubsection{Overestimation Bias}

Critic errors lead to overestimated Q-values ($\mathbb{E}[Q_{\phi}(s, a)] > Q^*(s, a)$). Actor exploits errors causing: aggressive control beyond safe limits, policy instability with oscillations, system overshoots, poor generalization, training divergence in later stages.

Khalid et al. (2022) showed load frequency control with DDPG overshoot 11.2\% (above limits), oscillation 2-3s (instability), slow settling 14s and  divergence after 1000+ episodes.

\subsubsection{High Variance in Learning}

The coupled actor-critic updates causes unstable policies  where critic changes affect actor, high Q-value variance due to moving target, hyperparameter over-sensitivity and inconsistent convergence.

\subsubsection{Hyperparameter Sensitivity}

DDPG requires careful tuning of learning rates ($\alpha_{\mu}$, $\alpha_Q$), target update rate ($\tau$), noise parameters ($\sigma$, $\theta$), network architecture (layers, neurons), training parameters (batch size, buffer size). Small changes (factor 2-3) can mean failure vs success.

% ============================================================================
\section{Twin Delayed Deep Deterministic Policy Gradient}

\subsection{TD3 Algorithm Development}

\subsubsection{Motivation and Origins}

Fujimoto et al. (2018) \cite{Fujimoto2018} introduced TD3 to address function approximation errors in actor-critic methods, building upon DDPG with three critical improvements.

\subsection{Three Key Innovations}

\subsubsection{Innovation 1: Clipped Double Q-Learning}

\textbf{Problem in DDPG:}
Single critic $Q_{\phi}$ tends to overestimate:
\begin{equation}
\mathbb{E}[Q_{\phi}(s, \mu_{\theta}(s))] \geq Q^{\mu}(s, \mu_{\theta}(s))
\end{equation}

\textbf{TD3 Solution:}
Use TWO independent critics $Q_{\phi_1}$ and $Q_{\phi_2}$:
\begin{equation}
y = r + \gamma \min_{i=1,2} Q_{\phi_i'}(s', \tilde{a}')
\end{equation}

where $\tilde{a}' = \mu_{\theta'}(s') + \epsilon$, $\epsilon \sim \text{clip}(\mathcal{N}(0, \sigma), -c, c)$

Rationale: minimum provides lower bound (conservative vs optimistic), reduces overestimation. Both critics trained independently (different initializations, sampling), minimum provides robustness to individual errors.

\textbf{Mathematical Justification:}
If critics have independent errors $\epsilon_1$ and $\epsilon_2$:
\begin{align}
Q_{\phi_1}(s, a) &= Q^*(s, a) + \epsilon_1 \\
Q_{\phi_2}(s, a) &= Q^*(s, a) + \epsilon_2
\end{align}

Then:
\begin{equation}
\mathbb{E}[\min(Q_{\phi_1}, Q_{\phi_2})] \approx Q^*(s, a) + \mathbb{E}[\min(\epsilon_1, \epsilon_2)] < Q^*(s, a) + \mathbb{E}[\epsilon_i]
\end{equation}

\subsubsection{Innovation 2: Delayed Policy Updates}

DDPG problem: updating actor every step causes policy to chase moving Q-target, resulting in high variance and instability.

\textbf{TD3 Solution:}
Update actor (and targets) every $d$ critic updates (typically $d = 2$):

\begin{algorithm}[H]
\caption{TD3 Update Schedule}
\begin{algorithmic}
\FOR{each training iteration $t$}
    \STATE Update both critics $Q_{\phi_1}$ and $Q_{\phi_2}$
    \IF{$t \mod d = 0$}
        \STATE Update actor $\mu_{\theta}$
        \STATE Update target networks $\theta'$, $\phi_1'$, $\phi_2'$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

Benefits: critics converge to better estimates before policy update, reduces actor-critic coupling, lowers variance, improves stability.

\subsubsection{Innovation 3: Target Policy Smoothing}

Problem: deterministic policies exploit narrow Q-function peaks (approximation errors), overfitting to spurious values, resulting in brittle policies with poor generalization.

\textbf{TD3 Solution:}
Add clipped noise to target action:
\begin{equation}
\tilde{a}' = \text{clip}(\mu_{\theta'}(s') + \text{clip}(\epsilon, -c, c), a_{low}, a_{high})
\end{equation}

where $\epsilon \sim \mathcal{N}(0, \sigma)$, typically $\sigma = 0.2$, $c = 0.5$

Effect: smooths value estimates around target action (not single point), regularizes similar to supervised learning, improves robustness to Q-approximation errors, prevents narrow peak exploitation.



\subsection{TD3 in Power System Applications}

\subsubsection{DFIG Wind Turbine Control}

Zholtayev et al. (2024) applied TD3 for virtual inertia and damping control in DFIG systems:

\textbf{Control Objectives:}
\begin{enumerate}
    \item Provide virtual inertia to grid
    \item Dampen power oscillations
    \item Maintain MPPT operation
    \item Ensure stability during transients
\end{enumerate}

\textbf{Physics-Constrained TD3:} Physical limits in reward function, constraint satisfaction via clipping/projection, model-based initialization for warm start.

\textbf{Results:} nadir et al. showed Frequency stability by +40\% vs conventional, RoCoF reduced by 35\%, oscillation damping at +50\% vs DDPG, robustness to 30\% parameter variations and converged in 2000 episodes (≈10 hours).


\subsubsection{Load Frequency Control}

Khalid et al. (2022) TD3 vs DDPG for LFC: two-area system, 40\% wind/solar, random loads.

\textbf{Performance:} Overshoot/undershoot/settling: GA-PID (0.9\%/-0.9\%/18.5s), PSO-PID (1.0\%/-1.0\%/16.2s), DDPG (0.3\%/-1.1\%/14.1s), TD3 (0.15\%/-0.6\%/7.5s). TD3: 50\% lower overshoot, 47\% faster settling vs DDPG, more consistent, no oscillations.

\subsubsection{Converter Control}

Muktiadji et al. (2024) TD3 for DC-DC boost converter: PI gain optimization, voltage regulation ±0.5\% (vs ±2\% PSO-PI), rise time 8 ms (vs 15 ms PI), zero overshoot (vs 5\% PSO-PI), robust to 20\% voltage variations.

\subsubsection{Microgrid Stability}

Lee et al. (2023) TD3 dynamic droop control for AC microgrids: adaptive coefficients, frequency < 0.1 Hz (vs 0.3 Hz fixed), voltage ±1.5\% (vs ±3\% fixed), power sharing < 2\% error.

\subsubsection{Recent TD3 Advances (2024--2025)}

\textbf{Virtual Power Plant Coordination \cite{VPP_TD3_2025}:} TD3-based DER coordination: wind/solar/storage aggregation, multi-objective optimization, 16.7\% cost reduction and 84.71\% higher mean reward vs DDPG baselines, scales to 100+ units.

\textbf{Multi-Agent TD3 for AGC \cite{MATD3_AGC_2025}:} Decentralized multi-area control: independent agents per area, cooperative learning via shared critics, frequency < 0.05 Hz, 40\% faster response vs centralized.

\textbf{Optimal Voltage-Frequency Control \cite{OptimalVoltageFrequency2025}:} Coordinated control for renewable grids: UPFC + EV integration, 60--80\% RES penetration, handles variability and uncertainty.

\textbf{Emerging Trends in TD3 Applications:}
\begin{enumerate}
    \item \textbf{Multi-Agent Extensions:} Scalable coordination of distributed resources
    \item \textbf{Hybrid Architectures:} Combining TD3 with MPC for constraint satisfaction
    \item \textbf{Transfer Learning:} Pre-trained policies adapted to new systems
    \item \textbf{Safety Enhancements:} Constrained TD3 with formal guarantees
    \item \textbf{Real-Time Deployment:} FPGA and edge computing implementations
\end{enumerate}

\subsubsection{TD3 Actor-Critic Architecture}

The TD3 architecture with twin critic networks is illustrated in Figure~\ref{fig:td3_actor_critic}, which shows how the dual critic approach mitigates overestimation bias by taking the minimum Q-value estimate for policy updates.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{images/TD3ActorCritic.png}
    \caption{TD3 actor-critic network architecture }
    \label{fig:td3_actor_critic}
\end{figure}

\subsection{Comparative Analysis: DDPG vs. TD3}

Both algorithms share the deterministic policy gradient foundation, but TD3's three innovations directly address DDPG's known weaknesses. Twin critics reduce Q-value overestimation bias---Khalid et al. (2022) \cite{Khalid2022} demonstrated 50\% overshoot reduction and 47\% settling time improvement when replacing DDPG with TD3 for load frequency control. Delayed policy updates prevent rapid policy changes based on noisy Q-estimates, reducing policy oscillation during training. Target policy smoothing improves generalization and reduces sensitivity to hyperparameter choices, enhancing overall robustness to parameter variations. However, DDPG remains computationally simpler during training (single critic, no delay scheduling) and may be preferable for real-time adaptive control scenarios where training overhead is critical. A systematic review of RL-based microgrid control \cite{Waghmare2025} confirms that while TD3 generally outperforms DDPG in stability-critical applications, the choice depends on specific deployment constraints including computational burden and real-time requirements.

\subsection{Beyond TD3: Advanced Actor-Critic Methods}

\subsubsection{Soft Actor-Critic (SAC)}

While DDPG and TD3 use deterministic policies, Soft Actor-Critic (SAC) introduces entropy regularization for enhanced exploration and robustness.

\textbf{Inclusion of Maximum Entropy RL:}
\begin{equation}
J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(s_t, a_t) \sim \rho_{\pi}} [r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot | s_t))]
\end{equation}

where $\mathcal{H}(\pi(\cdot | s_t)) = -\mathbb{E}_{a \sim \pi}[\log \pi(a|s_t)]$ is the entropy, encouraging exploration.

\textbf{SAC Architecture:} Stochastic policy $a_t \sim \pi_{\theta}(\cdot | s_t)$ (Gaussian), twin Q-networks (like TD3), automatic entropy tuning ($\alpha$), off-policy with replay.

\textbf{VSG Control with SAC \cite{SAC_VSG_2024}:} Virtual synchronous generators: frequency < 0.08 Hz, 30\% better transients vs TD3, robust to noise.

\textbf{Multi-Agent SAC for Voltage \cite{MASAC_Voltage_2024}:} Distribution voltage regulation: decentralized execution/centralized training, voltage < 1\%, handles 50\% PV.

\textbf{SAC vs TD3:} Policy (deterministic/stochastic), exploration (noise/entropy), noise robustness (good/excellent), sample efficiency (high/moderate), stability (very high/high), tuning (easier/complex).

\textbf{Microgrid Energy Management:} Improved SAC algorithms \cite{Yu2024} achieve 51.20\%, 52.38\%, and 13.43\% profit increases over DQN, SARSA, and PPO respectively for microgrid optimization. SAC's application to integrated energy systems with electricity, heat, and hydrogen storage \cite{Liang2024} demonstrates model-free control for multi-energy systems with complex energy coupling---relevant for future DFIG-PV systems integrated with storage.

\textbf{SAC use case:} High noise, stochastic dynamics, exploration-heavy, multi-modal policies.

\textbf{TD3 use case:} Deterministic dynamics, sample efficiency critical, faster convergence, tight computational constraints.

\subsubsection{Other Actor-Critic Variants}

\textbf{Proximal Policy Optimization (PPO):}
On-policy method with clipped objective, widely used but lower sample efficiency than off-policy methods for power systems.

\textbf{Multi-Agent Deep Deterministic Policy Gradient (MADDPG):}
Extension of DDPG for multi-agent settings, suitable for coordinated control of multiple generators.

\textbf{Distributed Distributional DDPG (D4PG):}
Combines distributional RL with DDPG, learning value distribution rather than expected value.

\subsubsection{Multi-Objective Deep RL for Wind Turbines}

Actor-critic deep RL enables multi-objective optimization beyond simple MPPT \cite{Frutos2025}. For 2.3 MW wind turbines, deep RL balances power maximization with operational constraints such as noise mitigation, mechanical stress reduction, and grid code compliance. This multi-objective capability is essential for commercial DFIG installations where regulatory compliance, equipment lifespan, and power quality must be simultaneously optimized.

\subsubsection{Quantum-Inspired Actor-Critic Methods}

Emerging quantum-inspired approaches \cite{Shen2024} demonstrate improved performance for frequency regulation in renewable-dominated microgrids. Quantum-inspired maximum entropy actor-critic (QISMEAC) algorithms outperform standard SAC and DDPG for handling renewable intermittency, offering potential pathways for next-generation control systems capable of managing high-penetration renewable grids with minimal frequency deviations (±0.2 Hz IEEE 1547 requirement).

\subsection{Safety-Constrained Reinforcement Learning}

A critical challenge for deploying DRL controllers in power systems is ensuring safety during both training and deployment. Standard DRL algorithms including DDPG and TD3 rely on unconstrained exploration that may produce actions violating operational limits potentially causing equipment damage or grid instability. Recent reviews \cite{Waghmare2025, Li2024} identify safe RL as an emerging research priority for power system applications. Approaches include constrained Markov decision processes (CMDPs) with Lagrangian relaxation to enforce voltage and frequency limits, barrier function-based reward shaping that penalizes proximity to constraint boundaries, and physics-constrained architectures such as the physics-constrained TD3 demonstrated by Zholtayev et al. (2024) \cite{PhysicsConstrainedTD32024} for DFIG virtual inertia control. Despite these advances, formal safety guarantees for DRL controllers in grid-connected renewable systems remain an open problem, particularly for the tightly coupled dynamics of DFIG-PV hybrid systems.

\subsection{Simulation-to-Real Transfer Challenges}

While DRL controllers achieve strong performance in simulation environments, bridging the gap to real-world deployment remains a significant challenge \cite{Ejiyi2025}. Key issues include: (i) model fidelity---simulation models may not capture all parasitic effects, switching transients, and sensor noise present in physical hardware; (ii) computational latency---neural network inference must meet real-time constraints (typically < 100 $\mu$s for power electronic control loops); and (iii) distribution shift---operating conditions encountered in deployment may differ from training distributions. Hardware-in-the-loop (HIL) testing platforms such as OPAL-RT provide an intermediate validation step between pure simulation and field deployment, enabling real-time execution with realistic signal conditioning and communication delays \cite{Pandey2022PIICON}. However, systematic methodologies for simulation to real transfer of DRL controllers in power systems remains largely unexplored.

\subsection{Summary of DRL Approaches for Power Systems}

Recent comprehensive reviews \cite{Li2024, Tang2024, Waghmare2025} identify several fundamental advantages of deep reinforcement learning for DFIG-PV hybrid systems. Unlike model predictive control (MPC) \cite{Joshal2023, Kamal2022}, DRL directly learns optimal policies from nonlinear system dynamics without requiring linearization or explicit system models. DRL controllers trained with diverse operating conditions demonstrate robust performance across parameter uncertainties inherent in renewable energy systems including turbine inertia variations, PV array degradation, and grid impedance changes \cite{Ejiyi2025}. Actor-critic methods naturally handle continuous control signals such as rotor-side converter voltages and grid-side converter modulation indices without discretization artifacts \cite{Lillicrap2015, Fujimoto2018}. Reward function design enables simultaneous optimization of multiple conflicting objectives including power maximization, grid stability, and fault ride-through capability \cite{Frutos2025}.

These advantages position TD3 and DDPG as promising alternatives to traditional PI/PID cascades and model-based controllers for next-generation hybrid renewable energy systems \cite{Sofian2024, AbdulMajeed2025}.

% ============================================================================
\section{Research Gaps and Opportunities}

\subsection{Identified Research Gaps}



\subsubsection{Gap 1: Unified Control of Tightly Coupled Systems}

\textbf{Current State :} 
\begin{itemize}
    \item Focus are on single sources wind or solar with limited DFIG-PV coupling investigation. 
    \item Separate subsystem controllers with coordination layers (suboptimal).
    \item Limited adoption of Unified frameworks.
\end{itemize}

\textbf{Challenge:} Strong coupling: PV→DC link→rotor converter→stator power; GSC balances both sources.

\textbf{Opportunity:} Unified RL agent observes all states, coordinates actions, optimizes overall performance, learns coupling implicitly.

\subsubsection{Gap 2: DRL Algorithm Comparison}

\textbf{Current State:} Comparisons mostly exist with PI baselines, there are limited DRL comparisons under identical conditions. 

\textbf{Opportunity:} Performance comparison of DDPG vs TD3 on identical systems


\subsection{Research Opportunities Addressed}

\subsubsection{Opportunity 1: Unified Control Framework}

Single unified TD3 controller managing RSC, GSC, and PV simultaneously. Learns coupled dynamics end-to-end, eliminates coordination layer, optimizes overall performance. 11-D state: $i_{qs}$, $i_{ds}$, $i_{qr}$, $i_{dr}$, $v_{dc}$, $i_{pv}$, $P_s$, $Q_s$, $P_g$, $Q_g$, $\theta_r$.

\subsubsection{Opportunity 2: Detailed DDPG vs. TD3 Comparison}

Side-by-side comparison on identical DFIG-PV system with same reward, state, action spaces.

\subsubsection{Opportunity 3: Hardware-in-Loop Validation}

OPAL-RT HIL validation: 10 microsecond switching dynamics, realistic constraints, demonstrates real-time feasibility and identifies implementation challenges.



% 

% ============================================================================
% END OF CHAPTER 2
% ============================================================================