
\section{Introduction}

This chapter presents the implementation of DDPG \cite{Lillicrap2015} and TD3 \cite{Fujimoto2018} algorithms for unified control of the hybrid DFIG-Solar PV system. The theoretical foundations of deep reinforcement learning including the MDP formulation, policy gradient methods, actor-critic architecture, experience replay, and target networks are detailed in Chapter~\ref{chap:literature}. The chapter focuses on the proposed control framework design and algorithm-specific implementation decisions.

The RL agent-environment interaction for the DFIG-PV system where the agent (DDPG or TD3 controller) observes system states and outputs converter voltage references, receiving reward feedback based on power tracking and voltage regulation performance.


The actor-critic architecture (Figure~\ref{fig:actor_critic_general}) forms the basis for both algorithms, where the actor network learns the deterministic control policy $\mu_\theta(s)$ and the critic network(s) evaluate action quality via the Q-function $Q_\phi(s,a)$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/Actor_Critic_Diagram.png}
    \caption{General actor-critic architecture}
    \label{fig:actor_critic_general}
\end{figure}

\section{Proposed Unified Control Framework}
\label{sec:unified_framework}

This section presents common design elements shared by DDPG and TD3: state space, action space, system dynamics, reward function, and training infrastructure.

\subsection{State Space Design}
\label{subsec:state_space}

11-dimensional state vector for complete observability:

\begin{equation}
\mathbf{s} = [i_{qs}, i_{ds}, i_{qr}, i_{dr}, v_{dc}, i_{pv}, P_s, Q_s, P_g, Q_g, \theta_r]^T \in \mathbf{R}^{11}
\label{eq:state_vector}
\end{equation}

\textbf{Components:} DFIG electrical ($i_{qs}, i_{ds}, i_{qr}, i_{dr}$), DC link/PV ($v_{dc}, i_{pv}$), power flow ($P_s, Q_s, P_g, Q_g$), mechanical ($\theta_r$). This captures wind turbine, PV, and grid subsystem couplings for coordinated control.

\subsection{Action Space Design}
\label{subsec:action_space}

4-dimensional continuous action vector: d-q axis voltage references for RSC and GSC:

\begin{equation}
\mathbf{a} = [v_{qr}, v_{dr}, v_{qg}, v_{dg}]^T \in \mathbf{R}^4
\label{eq:action_vector}
\end{equation}

\textbf{Components:} RSC ($v_{qr}, v_{dr}$): torque and rotor reactive power. GSC ($v_{qg}, v_{dg}$): DC link voltage and grid power.

\textbf{Scaling:} NN outputs $[-1, 1]$ (tanh) scaled to physical limits:

\begin{equation}
v_{actual} = v_{min} + \frac{(a_{normalized} + 1)}{2} \times (v_{max} - v_{min})
\label{eq:action_scaling}
\end{equation}

Typical: $v_{min} = -400$ V, $v_{max} = +400$ V. Continuous actions enable fine-grained control, minimizing electrical stress.

\subsection{System Dynamics Representation}
\label{subsec:system_dynamics}

System state evolution governed by nonlinear differential equations (Chapter~\ref{chap:rl}):

\begin{equation}
\dot{\mathbf{s}} = f(\mathbf{s}, \mathbf{a}) = [\dot{i}_{qs}, \dot{i}_{ds}, \dot{i}_{qr}, \dot{i}_{dr}, \dot{\omega}_r, \dot{v}_{dc}, \dot{i}_{pv}, \dot{P}_s, \dot{Q}_s, \dot{P}_g, \dot{Q}_g, \dot{\theta}_r]^T
\label{eq:system_dynamics}
\end{equation}

\textbf{Key Components:}

\paragraph{Stator Currents:}

\begin{equation}
\dot{i}_{qs} = \frac{\omega_b}{L'_s} \left( -R_1 i_{qs} + \omega_s L'_s i_{ds} + \frac{\omega_r}{\omega_s} e'_{qs} - \frac{1}{T_r \omega_s} e'_{ds} - v_{qs} + \frac{L_m}{L_{rr}} v_{qr} \right)
\label{eq:iqs_dot}
\end{equation}

\begin{equation}
\dot{i}_{ds} = \frac{\omega_b}{L'_s} \left( -\omega_s L'_s i_{qs} - R_1 i_{ds} + \frac{1}{T_r \omega_s} e'_{qs} + \frac{\omega_r}{\omega_s} e'_{ds} - v_{ds} + \frac{L_m}{L_{rr}} v_{dr} \right)
\label{eq:ids_dot}
\end{equation}

\paragraph{Rotor Speed:}
\begin{equation}
\dot{\omega}_r = \frac{1}{J} (T_m - T_e - B\omega_r)
\label{eq:omega_r_dot}
\end{equation}

\paragraph{DC Link Voltage:}
\begin{equation}
\dot{v}_{dc} = \frac{1}{C} (i_{pv} + i_{r,dc} - i_{g,dc})
\label{eq:vdc_dot}
\end{equation}

These coupled nonlinear dynamics motivate deep RL for learning optimal control from system interactions.

\subsection{Reward Function Structure}
\label{subsec:reward_function}

Multi-objective quadratic penalty balancing power tracking, voltage regulation, and stability.

\subsubsection{RSC Reward}

\begin{equation}
r_{RSC} = -(w_1 (\omega_r - \omega_r^*)^2 + w_2 (P_s - P_s^*)^2 + w_3 (Q_s - Q_s^*)^2)
\label{eq:r_rsc}
\end{equation}

\subsubsection{GSC Reward}

\begin{equation}
r_{GSC} = -(w_4 (v_{dc} - v_{dc}^*)^2 + w_5 (P_g - P_g^*)^2 + w_6 (Q_g - Q_g^*)^2)
\label{eq:r_gsc}
\end{equation}

\subsubsection{Total Reward}

\begin{equation}
r = r_{RSC} + r_{GSC}
\label{eq:total_reward}
\end{equation}

This unified formulation simultaneously optimizes both converters via shared DC link.

\subsubsection{Reward Weights}

Baseline weights (determined experimentally):
\begin{itemize}
    \item $w_1 = 0.4$ (frequency regulation)
    \item $w_2 = 0.3$ (rotor active power tracking)
    \item $w_3 = 0.3$ (rotor reactive power control)
    \item $w_4 = 0.2$--$0.3$ (DC link voltage regulation)
    \item $w_5 = 0.2$--$0.4$ (grid active power)
    \item $w_6 = 0.2$--$0.4$ (grid reactive power)
\end{itemize}

Minor $w_4$, $w_5$, $w_6$ variations between DDPG/TD3 (Sections~\ref{subsec:ddpg_rewards}, \ref{subsec:td3_rewards}).

\subsection{Neural Network Foundation}
\label{subsec:nn_foundation}

DDPG and TD3 use deep neural networks for actor/critic approximation.

\subsubsection{Actor Network}

Maps states to actions: $\mu(s|\theta_\mu) = \tanh(W_n\sigma(W_{n-1} \cdots \sigma(W_1 s + b_1) \cdots + b_{n-1}) + b_n)$ where $\sigma(x) = \max(0, x)$ (ReLU).

\textbf{Architecture:} It consists of input layer of [11] neurons with [400x300] hidden layers and output layers with [4] neurons with ReLU activation and tanh output.

\subsubsection{Critic Network}

Evaluates state-action pairs: $Q(s,a|\theta_Q) = W'_n\sigma(W'_{n-1} \cdots \sigma(W'_1[s,a] + b'_1) \cdots + b'_{n-1}) + b'_n$ where $[s,a]$ concatenates state/action.

\textbf{Architecture:} It consists of input layer of [15] neurons with [400x300] hidden layers and output layers with [1] neurons with ReLU activation and linear output. DDPG uses 1 critic, TD3 uses 2 (Section~\ref{subsec:td3_critics}).

\subsubsection{Weight Initialization}

All hidden layers use He (Kaiming) initialization \cite{Lillicrap2015}: weights drawn from $\mathcal{U}(-\frac{1}{\sqrt{f_{in}}}, \frac{1}{\sqrt{f_{in}}})$ where $f_{in}$ is the fan-in of each layer. The actor output layer uses a uniform initialization from $\mathcal{U}(-3 \times 10^{-3}, 3 \times 10^{-3})$ to ensure initial actions are near zero, preventing large initial control signals that could destabilize the learning process during early training episodes.

\subsubsection{State Normalization}

The 11 state components span different physical scales (currents: 0--50 A, voltages: 0--800 V, powers: 0--10 kW, angle: 0--$2\pi$ rad). Without normalization, high-magnitude features dominate gradient updates, degrading learning. Min-max normalization is applied:
\begin{equation}
\hat{s}_i = \frac{s_i - s_{i,\min}}{s_{i,\max} - s_{i,\min}} \in [0, 1]
\label{eq:state_normalization}
\end{equation}
where $s_{i,\min}$ and $s_{i,\max}$ are the expected operating ranges for each state variable, determined from the system rated parameters. Running statistics are updated during training to adapt to encountered state distributions. No batch normalization is used within the network layers, as recent studies show batch normalization suffers from non-stationarity under RL exploration; the min-max input normalization provides sufficient conditioning.

\subsubsection{Gradient Clipping}

To prevent training instability from large gradient magnitudes particularly during early training when reward signals are noisy L2 norm gradient clipping is applied to both actor and critic networks:
\begin{equation}
\hat{g} = \frac{g}{\max(1, \|g\|_2 / g_{\max})}
\label{eq:gradient_clipping}
\end{equation}
where $g_{\max} = 1.0$ for critic networks and $g_{\max} = 0.5$ for the actor network. The more conservative actor clipping prevents aggressive policy updates from destabilizing the learned control strategy.

\subsection{Training Infrastructure}
\label{subsec:training_infrastructure}

\subsubsection{Platform}

\textbf{Software:} Python 3.8, TensorFlow 2.10.0/Keras, NumPy 1.23.0, OpenAI Gym 0.26.0, MATLAB R2021b (HIL).

\textbf{Hardware:} Google Colab Pro (NVIDIA Tesla T4, 16 GB VRAM) for training; OPAL-RT OP4510 for HIL validation.
-------------------TBD------------------------
\textbf{Recent Advances:} MATLAB/Simulink optimization \cite{Mauludin2025,Savio2025}, digital twins \cite{AlShetwi2025}, techno-economic frameworks \cite{Ayua2024}, multi-objective simulation \cite{Jacob2025}.
-----------------------------------
\subsubsection{Experience Replay}

Replay buffer $\mathcal{B} = \{(s_t, a_t, r_t, s_{t+1})\}_{t=1:T}$ stores transitions. Random mini-batches sampled to break temporal correlations.

\textbf{Configuration:} Capacity $1 \times 10^6$, batch size 64, uniform sampling.

\subsubsection{Target Networks}

Soft updates stabilize learning:
\begin{equation}
\theta'_Q \leftarrow \tau \theta_Q + (1-\tau) \theta'_Q, \quad
\theta'_\mu \leftarrow \tau \theta_\mu + (1-\tau) \theta'_\mu
\label{eq:target_updates}
\end{equation}
where $\tau \ll 1$ prevents rapid target changes.

\subsubsection{Curriculum Learning}

Gradual task difficulty increase: \textbf{Phase 1 (Eps 1--100):} Random exploration, buffer population. \textbf{Phase 2 (101--500):} DC link voltage only ($r = -(v_{dc} - v_{dc}^*)^2$). \textbf{Phase 3 (501--1000):} DC link + RSC power ($r = r_{RSC} + r_{GSC,voltage}$). \textbf{Phase 4 (1001+):} Full multi-objective (Eq.~\ref{eq:total_reward}). Essential for convergence.

\subsubsection{Reproducibility Protocol}

All experiments are conducted over 5 independent training runs with different random seeds (seeds 0--4) for network initialization, replay buffer sampling, and exploration noise generation. Results are reported as mean $\pm$ standard deviation across these runs. The simulation environment is deterministic given the same seed, ensuring reproducibility. Model checkpoints are saved every 100 episodes, and the best-performing checkpoint (highest validation reward) is selected for HIL deployment.

\subsubsection{Proposed Emergency Fallback Strategy}

For practical deployment of DRL controllers in power systems, a safety fallback mechanism is recommended. In the proposed design, a supervisory monitor would check DRL controller outputs against physical constraints---including converter voltage limits ($|v| > v_{max}$), DC link voltage deviation beyond $\pm$10\% of nominal, or sustained reward degradation over consecutive steps. Upon violation, control would transfer to a conventional PI controller with pre-tuned gains. While this fallback is not implemented in the current simulation framework, it represents an important consideration for future real-world deployment of the proposed DRL-based controllers.
-------------------------tbd----------------
\subsection{Summary of Common Elements}

These elements---state/action spaces, dynamics, reward, networks, infrastructure, and normalization---form the unified foundation for DDPG and TD3.
-------------------------------------------

% ------------------------------------------------------------
% SECTION 5.2: DDPG IMPLEMENTATION
% ------------------------------------------------------------

\subsection{Deep Deterministic Policy Gradient (DDPG) Implementation}
\label{sec:ddpg_implementation}

DDPG algorithm for hybrid DFIG-PV system \cite{Pandey2025DDPG}: actor-critic for continuous control with deterministic policy gradients.

\subsubsection{DDPG Overview}
\label{subsec:ddpg_overview}

Extends DQN to continuous actions. \textbf{Core Principles:} Deterministic policy (simplifies gradient), off-policy learning (replay buffer, sample efficiency), actor-critic (actor proposes, critic evaluates), continuous action space (direct voltage commands, fine-grained control).

\subsubsection{Single Critic Architecture}
\label{subsec:ddpg_critic}

Single critic $Q_\phi(s,a)$ estimates action-value: $Q^\pi(s,a) = E\left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid s_0=s, a_0=a, \pi \right]$. Structure per Section~\ref{subsec:nn_foundation}.

\subsubsection{Policy Gradient}
\label{subsec:ddpg_policy_gradient}

Deterministic policy gradient: $\nabla_{\theta_\mu} J = E_{s \sim \rho^\beta} \left[ \nabla_a Q(s,a|\phi) \big|_{a=\mu(s)} \nabla_{\theta_\mu} \mu(s|\theta_\mu) \right]$.

Mini-batch approximation: $\nabla_{\theta_\mu} J \approx \frac{1}{N} \sum_{i=1}^{N} \nabla_a Q(s_i,a|\phi) \big|_{a=\mu(s_i)} \nabla_{\theta_\mu} \mu(s_i|\theta_\mu)$.

\subsubsection{Target Value Computation}
\label{subsec:ddpg_target}

The critic is trained using temporal difference learning with target values computed from the target networks:

\begin{equation}
y_i = r_i + \gamma Q'(s_{i+1}, \mu'(s_{i+1}|\theta'_\mu)|\phi')
\label{eq:ddpg_target}
\end{equation}

The critic loss function is then:

\begin{equation}
L(\phi) = \frac{1}{N} \sum_{i=1}^{N} (y_i - Q(s_i, a_i|\phi))^2
\label{eq:ddpg_critic_loss}
\end{equation}

\subsubsection{Exploration Strategy}
\label{subsec:ddpg_exploration}

To ensure sufficient exploration during training, DDPG adds noise to the deterministic policy:

\begin{equation}
a_t = \mu(s_t|\theta_\mu) + \mathcal{N}_t
\label{eq:ddpg_exploration}
\end{equation}

where $\mathcal{N}_t$ is noise generated by an Ornstein-Uhlenbeck process:

\begin{equation}
d\mathcal{N}_t = \theta_{OU}(\mu_{OU} - \mathcal{N}_t)dt + \sigma_{OU} dW_t
\label{eq:ou_process}
\end{equation}

with parameters $\theta_{OU} = 0.15$ (mean reversion rate), $\mu_{OU} = 0.0$ (long-term mean), and $\sigma_{OU} = 0.2$ (volatility).

The Ornstein-Uhlenbeck process generates temporally correlated noise, which is more suitable for physical control systems than uncorrelated Gaussian noise.

\subsubsection{DDPG Training Procedure}

The complete DDPG training procedure follows these steps:

\textbf{Initialization:}
\begin{enumerate}
    \item Initialize actor $\mu(s|\theta_\mu)$ and critic $Q(s,a|\phi)$ with random weights
    \item Initialize target networks: $\theta'_\mu \leftarrow \theta_\mu$, $\phi' \leftarrow \phi$
    \item Initialize replay buffer $\mathcal{B}$ with capacity $N_{buffer}$
    \item Initialize Ornstein-Uhlenbeck noise process $\mathcal{N}$
\end{enumerate}

\textbf{Training Loop (for each episode):}
\begin{enumerate}
    \item Reset environment and receive initial state $s_1$
    \item Reset noise process $\mathcal{N}$
    \item For each timestep $t = 1$ to $T$:
    \begin{enumerate}
        \item Select action with exploration: $a_t = \mu(s_t|\theta_\mu) + \mathcal{N}_t$
        \item Clip action to valid range: $a_t \leftarrow \text{clip}(a_t, a_{low}, a_{high})$
        \item Execute action $a_t$ in environment
        \item Observe reward $r_t$ and next state $s_{t+1}$
        \item Store transition $(s_t, a_t, r_t, s_{t+1})$ in $\mathcal{B}$
        \item If $|\mathcal{B}| \geq N_{batch}$:
        \begin{enumerate}
            \item Sample random mini-batch of $N_{batch}$ transitions from $\mathcal{B}$
            \item Compute target Q-values: $y_i = r_i + \gamma Q'(s_{i+1}, \mu'(s_{i+1}|\theta'_\mu)|\phi')$
            \item Update critic by minimizing: $L = \frac{1}{N_{batch}}\sum_i (y_i - Q(s_i,a_i|\phi))^2$
            \item Update actor using policy gradient
            \item Update target networks with soft updates: $\phi' \leftarrow \tau \phi + (1-\tau)\phi'$ and $\theta'_\mu \leftarrow \tau \theta_\mu + (1-\tau)\theta'_\mu$
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

\subsubsection{DDPG Hyperparameters}
\label{subsec:ddpg_hyperparameters}

\textbf{Learning:} Discount factor $\gamma = 0.99$, actor learning rate $1 \times 10^{-4}$, critic learning rate $1 \times 10^{-3}$, target update rate $\tau = 0.001$.

\textbf{Training:} Batch size 64, replay buffer $1 \times 10^6$, Ornstein-Uhlenbeck noise ($\theta = 0.15$, $\sigma = 0.2$).

\subsubsection{DDPG Reward Weights}
\label{subsec:ddpg_rewards}

\textbf{RSC weights:} $w_1 = 0.4$, $w_2 = 0.3$, $w_3 = 0.3$.

\textbf{GSC weights:} $w_4 = 0.3$, $w_5 = 0.2$, $w_6 = 0.2$.

\subsubsection{DDPG Training Configuration}
\label{subsec:ddpg_training_config}

\textbf{Episodes:} 2000 episodes × 1000 steps each.

\textbf{Curriculum:} Warmup (episodes 1--100), simplified task (101--500), intermediate (501--1000), full multi-objective (1001--2000).

\textbf{Exploration:} Minimum noise 0.1, decay factor 0.995 per episode.

\textbf{Convergence:} Same criterion as Eq.~\ref{eq:convergence_criterion} with 100 consecutive episodes (less stringent than TD3). Total training time $\approx$8 hours.
\subsubsection{DDPG Training Results}
\label{subsec:ddpg_training_results}
The DDPG agent was trained for 2000 episodes (approximately $4 \times 10^{6}$ environment interaction steps) with actor learning rate $\alpha_{actor} = 1 \times 10^{-4}$, critic learning rate $\alpha_{critic} = 1 \times 10^{-3}$, discount factor $\gamma = 0.99$, and Ornstein-Uhlenbeck exploration noise. Figures~\ref{fig:ddpg_critic_loss} and~\ref{fig:ddpg_rewards} illustrate the critic loss and episode reward progression throughout training.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{images/DDPG_Critic_Loss.png}
    \caption{DDPG critic loss over training steps.}
    \label{fig:ddpg_critic_loss}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{images/DDPG_Episode_Rewards.png}
    \caption{DDPG episode reward progression.}
    \label{fig:ddpg_rewards}
\end{figure}

\subsubsection{Phase 1: Initial Convergence (Episodes 1--1000)}

During the first half of training (Steps 0 to $\approx 2 \times 10^{6}$), the critic loss (MSE) remains low and well-controlled, rising gradually from near zero to approximately $0.4 \times 10^{17}$. Concurrently, episode rewards remains at approximately $-1.15 \times 10^{11}$.The stable critic loss in this phase confirms that the single Q-network is accurately representing the value function.

\subsubsection{Phase 2: Exploration-Driven Improvement (Episodes 1001--2000)}

The critic loss increases to $\approx 1.5 \times 10^{17}$ before oscillating in the range $0.5$--$1.5 \times 10^{17}$ for the remainder of training. This pattern is a behavior of DDPG's single-critic architecture, wherein Q-value overestimation accumulates over training \cite{Fujimoto2018}. The increasing reward variance in Phase 2 reflects policy exploration adaptive response.



\subsubsection{DDPG Implementation Summary}
\label{subsec:ddpg_summary}

Solid baseline for DFIG-PV control. Key characteristics:

\textbf{Strengths:} Simple single-critic architecture, efficient training (≈8 hours), good power tracking, proven track record.

\textbf{Limitations:} Overestimation bias (Section~\ref{subsec:td3_motivation}), hyperparameter sensitivity, aggressive control/overshoot. These motivate TD3 development.


% ------------------------------------------------------------
% SECTION 5.3: TD3 IMPLEMENTATION
% ------------------------------------------------------------

\subsection{Twin-Delayed Deep Deterministic Policy Gradient (TD3) Implementation}
\label{sec:td3_implementation}

TD3 implementation \cite{Pandey2025TD3} addresses DDPG limitations via three innovations.

\subsubsection{TD3 Innovations Applied to DFIG-PV Control}
\label{subsec:td3_motivation}

As detailed in Chapter~\ref{chap:literature}, TD3 addresses DDPG's overestimation bias, policy-value coupling, and hyperparameter sensitivity through three innovations: clipped double Q-learning, target policy smoothing, and delayed policy updates. This section focuses on the implementation-specific details for the DFIG-PV system.

\subsubsection{Twin Critic Architecture}
\label{subsec:td3_critics}

TD3 employs two structurally identical but independently initialized critic networks:

\textbf{Critic 1:} $Q_{\phi_1}(s,a)$ with parameters $\phi_1 = \{W^{(1)}_i, b^{(1)}_i\}$

\textbf{Critic 2:} $Q_{\phi_2}(s,a)$ with parameters $\phi_2 = \{W^{(2)}_i, b^{(2)}_i\}$

Both critics follow the architecture described in Section~\ref{subsec:nn_foundation}, processing a 15-dimensional input formed by concatenating 11 states and 4 actions, passing through a first hidden layer of 400 neurons with ReLU activation and a second hidden layer of 300 neurons with ReLU activation, and producing a single scalar Q-value with linear activation.

\textbf{Total TD3 Critic Parameters:}
\begin{equation}
N_{params,TD3\_critics} = 2 \times 126,701 = 253,402
\end{equation}

This is exactly double the DDPG critic parameters, representing the additional computational cost.

\subsubsection{TD3 Target Value Computation}
\label{subsec:td3_target_computation}

The target value for critic training incorporates all three TD3 innovations:

\begin{equation}
y_i = r_i + \gamma \min_{j=1,2} Q'_{\phi_j}\left(s_{i+1}, \text{clip}\left(\mu'(s_{i+1}|\theta'_\mu) + \text{clip}(\epsilon, -c, c), a_{low}, a_{high}\right)\right)
\label{eq:td3_target_full}
\end{equation}

\textbf{Step-by-step computation:}
\begin{enumerate}
    \item Target actor produces action: $\mu'(s_{i+1}|\theta'_\mu)$
    \item Add clipped noise: $\tilde{a}' = \mu'(s_{i+1}) + \text{clip}(\epsilon, -c, c)$
    \item Ensure physical bounds: $\tilde{a}' = \text{clip}(\tilde{a}', a_{low}, a_{high})$
    \item Evaluate with both target critics: $Q'_{\phi_1}(s_{i+1}, \tilde{a}')$ and $Q'_{\phi_2}(s_{i+1}, \tilde{a}')$
    \item Take minimum: $\min(Q'_{\phi_1}, Q'_{\phi_2})$
    \item Add immediate reward: $y_i = r_i + \gamma \min(...)$
\end{enumerate}

\subsubsection{TD3 Policy Gradient with Delayed Updates}
\label{subsec:td3_policy_gradient}

The actor is updated using the policy gradient, but only every $d$ critic updates:

\begin{equation}
\nabla_{\theta_\mu} J = \frac{1}{N} \sum_{i=1}^{N} \nabla_a Q_{\phi_1}(s_i, a|_{a=\mu(s_i)}) \nabla_{\theta_\mu} \mu(s_i|\theta_\mu)
\label{eq:td3_policy_gradient}
\end{equation}

\textit{Note: Only the first critic $Q_{\phi_1}$ is used for the policy gradient, as using both would double the gradient computation cost without significant benefit.}

\subsubsection{TD3 Training Procedure}

The complete TD3 training procedure follows these steps:

\textbf{Initialization:}
\begin{enumerate}
    \item Initialize actor $\mu(s|\theta_\mu)$ and twin critics $Q_{\phi_1}, Q_{\phi_2}$ with random weights
    \item Initialize target networks: $\theta'_\mu \leftarrow \theta_\mu$, $\phi'_1 \leftarrow \phi_1$, $\phi'_2 \leftarrow \phi_2$
    \item Initialize replay buffer $\mathcal{B}$ with capacity $N_{buffer}$
    \item Initialize update counter $k \leftarrow 0$
\end{enumerate}

\textbf{Training Loop (for each episode):}
\begin{enumerate}
    \item Reset environment and receive initial state $s_1$
    \item For each timestep $t = 1$ to $T$:
    \begin{enumerate}
        \item Select action with exploration: $a_t = \mu(s_t|\theta_\mu) + \epsilon$, $\epsilon \sim \mathcal{N}(0, \sigma_{explore})$
        \item Clip action to valid range: $a_t \leftarrow \text{clip}(a_t, a_{low}, a_{high})$
        \item Execute action $a_t$ in environment
        \item Observe reward $r_t$ and next state $s_{t+1}$
        \item Store transition $(s_t, a_t, r_t, s_{t+1})$ in $\mathcal{B}$
        \item If $|\mathcal{B}| \geq N_{batch}$:
        \begin{enumerate}
            \item Sample random mini-batch of $N_{batch}$ transitions from $\mathcal{B}$
            \item Compute target actions with smoothing
            \item Compute target Q-values with clipping: $y_i \leftarrow r_i + \gamma \min_{j=1,2} Q'_{\phi_j}(s_{i+1}, \tilde{a}_{i+1})$
            \item Update both critics by minimizing losses
            \item Increment counter: $k \leftarrow k + 1$
            \item If $k \mod d = 0$ (Delayed policy update): Update actor using policy gradient (first critic only) and update target networks with soft updates
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

\subsubsection{TD3 Hyperparameters}
\label{subsec:td3_hyperparameters}

\textbf{Learning:} Discount factor $\gamma = 0.99$, actor learning rate $8 \times 10^{-5}$, critic learning rate $7.5 \times 10^{-4}$, target update rate $\tau = 0.0008$.

\textbf{Training:} Batch size 64, replay buffer $1 \times 10^6$.

\textbf{TD3-Specific:} Policy delay $d = 2$, target policy noise $\tilde{\sigma} = 0.2$, noise clip $c = 0.5$, exploration noise $\sigma_{explore} = 0.1$.

\subsubsection{TD3 Reward Weights}
\label{subsec:td3_rewards}

\textbf{RSC weights:} $w_1 = 0.4$, $w_2 = 0.3$, $w_3 = 0.3$.

\textbf{GSC weights:} $w_4 = 0.2$, $w_5 = 0.4$, $w_6 = 0.4$.

\subsubsection{TD3 Training Configuration}
\label{subsec:td3_training_config}

\textbf{Episodes:} 2500 episodes × 1600 steps each.

\textbf{Curriculum:} Warmup (episodes 1--100), simplified task (101--300), intermediate (301--800), full multi-objective (801--2500).

\textbf{Exploration Decay:} TD3 uses Gaussian exploration noise with linear decay: $\sigma_{explore}(e) = \max(0.01, \sigma_0 - e \times \Delta\sigma)$, where $\sigma_0 = 0.1$ and $\Delta\sigma = 4 \times 10^{-5}$ per episode, reaching minimum noise at approximately episode 2250.

\textbf{Convergence Criterion:} Training is considered converged when the 100-episode moving average of the total reward $\bar{R}_{100}$ satisfies:
\begin{equation}
\frac{|\bar{R}_{100}(e) - \bar{R}_{100}(e-100)|}{\max(|\bar{R}_{100}(e)|, 1)} < 0.005
\label{eq:convergence_criterion}
\end{equation}
for 200 consecutive episodes, indicating less than 0.5\% relative change in expected return. Convergence achieved around episode 2000. Total training time $\approx$12 hours.
\subsubsection{TD3 Training Results}
\label{subsec:td3_training_results}
The TD3 agent was trained for 2500 episodes with actor learning rate $\alpha_{actor} = 8 \times 10^{-5}$, critic learning rate $\alpha_{critic} = 7.5 \times 10^{-4}$, discount factor $\gamma = 0.99$, policy update delay $d = 2$, and target policy smoothing parameters $\sigma = 0.2$, $c = 0.5$. Figures~\ref{fig:td3_critic1_loss}, \ref{fig:td3_critic2_loss}, and~\ref{fig:td3_rewards} present the twin critic losses and episode rewards.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{images/TD3_Critic1_Loss.png}
    \caption{TD3 Critic 1 loss over training steps}
    \label{fig:td3_critic1_loss}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{images/TD3_Critic2_Loss.png}
    \caption{TD3 Critic 2 loss over training steps}
    \label{fig:td3_critic2_loss}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{images/TD3_Episode_Rewards.png}
    \caption{TD3 episode reward}
    \label{fig:td3_rewards}
\end{figure}

\subsubsection{Twin Critic Loss Dynamics}

Both Critic 1 and Critic 2 losses rise from zero to approximately $0.8$--$1.0 \times 10^{17}$ within the first $5 \times 10^{5}$ training steps before stabilizing. The clipped double Q-learning ensures that policy gradient targets are bounded by the minimum of the two Q-estimates. The  spikes in Critic 2 are corrected within a few thousand steps and do not impact the actor due to the delayed policy updates.

The two critics show independent but related loss curves during training. 
This behavior is desirable as diverse Q-function estimates speed up training convergence \cite{Fujimoto2018}.

\subsubsection{Episode Reward Convergence}

TD3 episode rewards exhibit a qualitatively different convergence profile compared to DDPG. After a brief exploratory spike at episodes 1--10, rewards rapidly stabilise near $-1.14 \times 10^{11}$ and maintain this level with very low variance throughout the remaining 2490 episodes. Key observations are:

\begin{enumerate}
    \item \textbf{Early stabilisation:} The policy converges to desired reward level within approximately 50 episodes, compared to the 1000-episode convergence delay observed in DDPG.
    \item \textbf{Low reward variance:} The stable reward trajectory demonstrates that target policy smoothing effectively regularises the policy consistent with Equation~(\ref{eq:target_smoothing}).
    \item \textbf{Delayed actor updates:} The policy delay ensures the actor network is only updated once the two critics have had two gradient steps to improve their value estimates.
\end{enumerate}
\subsubsection{TD3 Implementation Summary}
\label{subsec:td3_summary}

The TD3 implementation addresses DDPG's limitations through three complementary mechanisms:

\textbf{Strengths:}

The TD3 implementation offers significant advantages including reduced overestimation bias through the use of twin critics, more stable learning enabled by delayed policy updates, improved generalization via target policy smoothing, superior performance on voltage regulation tasks, and more conservative and safer control actions that reduce the risk of equipment damage.

\textbf{Trade-offs:}

However, TD3 requires 50\% more training (12h vs 8h), double critic parameters (253K vs 127K), slightly complex implementation, additional hyperparameters ($d$, $\tilde{\sigma}$, $c$). Performance improvements justify adoption for safety-critical applications.


\subsection{TD3 Ablation Study Methodology}
\label{subsec:ablation_methodology}

To quantify the individual contribution of each TD3 innovation to DFIG-PV control performance, an ablation study is conducted with the following configurations:

\begin{enumerate}
    \item \textbf{DDPG (Baseline):} Single critic, no delayed updates, no target smoothing
    \item \textbf{DDPG + Twin Critics (TC):} Add clipped double Q-learning only---isolates the effect of overestimation bias reduction on DC link voltage stability and power tracking
    \item \textbf{DDPG + Delayed Updates (DU):} Add delayed policy updates ($d=2$) only---isolates the effect of policy-value decoupling on training stability
    \item \textbf{DDPG + Target Smoothing (TS):} Add target policy smoothing ($\sigma=0.2$, $c=0.5$) only---isolates the effect of value function regularization on generalization
    \item \textbf{Full TD3 (TC + DU + TS):} All three innovations combined
\end{enumerate}

Each configuration is trained for 2500 episodes over 5 random seeds using identical hyperparameters (except for the ablated component). Evaluation metrics include: mean episode reward, DC link voltage regulation error ($\Delta v_{dc}$), active/reactive power tracking RMSE, overestimation bias ($\mathbb{E}[Q_\phi - Q^\pi]$ estimated via Monte Carlo rollouts), and convergence speed (episodes to reach 90\% of final performance). Results are presented in Chapter~\ref{chap:td3}.

% ------------------------------------------------------------
% SECTION 5.4: TRAINING RESULTS
% ------------------------------------------------------------




% ------------------------------------------------------------
% SECTION 5.4: IMPLEMENTATION COMPARISON
% ------------------------------------------------------------

\subsection{Comparative Implementation Analysis}
\label{sec:implementation_comparison}

Systematic DDPG vs TD3 comparison: architectural differences, computational trade-offs, deployment considerations.

\subsubsection{Algorithmic Differences}
\label{subsec:algo_differences}

\textbf{DDPG vs TD3:} \textbf{Architecture:} DDPG (1 critic, 252K params), TD3 (2 critics, 379K params). \textbf{Value estimation:} DDPG single $Q_\phi(s,a)$ (overestimation prone), TD3 $\min(Q_{\phi_1}, Q_{\phi_2})$ (bias mitigated). \textbf{Policy:} DDPG updates every step (deterministic), TD3 every $d$ steps (smoothed noise). \textbf{Hyperparameters:} DDPG (actor LR=$1 \times 10^{-4}$, critic LR=$1 \times 10^{-3}$, $\tau$=0.001), TD3 (actor LR=$8 \times 10^{-5}$, critic LR=$7.5 \times 10^{-4}$, $\tau$=0.0008, delay $d$=2). \textbf{Training:} DDPG (2000 episodes, 1000 steps, 8h), TD3 (2500 episodes, 1600 steps, 12h). \textbf{Performance:} TD3 +50\% computational cost, higher stability, less aggressive control.

\subsubsection{Comparative Training Analysis: DDPG vs.\ TD3}
\label{sec:training_comparitive_results}
\begin{table}[htbp]
\centering
\caption{Comparative training convergence characteristics: DDPG vs.\ TD3}
\label{tab:training_comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{DDPG} & \textbf{TD3} \\
\midrule
Training Episodes & 2000 & 2500 \\
Total Interaction Steps & $\approx 4 \times 10^{6}$ & $\approx 4 \times 10^{6}$ \\
Critic Architecture & Single Q-network & Twin Q-networks \\
Peak Critic Loss ($\times 10^{17}$) & $\approx 1.5$ & $\approx 1.75$ (Critic 2) \\
Episodes to Reward Stabilisation & $\approx 1000$ & $\approx 50$ \\
Converged Mean Reward ($\times 10^{11}$) & $\approx -0.85$ & $\approx -1.14$ \\
Reward Variance (converged) & High ($\pm 0.15 \times 10^{11}$) & Low ($< 0.02 \times 10^{11}$) \\
\bottomrule
\end{tabular}
\end{table}

The training results validate the use of TD3 over DDPG in this application. TD3's training stability supports HIL performance reported in Section~\ref{sec:experimental_runs}, where the TD3 policy outperforms DDPG across various test scenarios.
\subsubsection{Training Phase Complexity}

\textbf{DDPG per training step:}

Each DDPG training step requires $O(125,504)$ operations for the actor forward pass, $O(126,701)$ operations for the critic forward pass, $O(252,205)$ operations for backward passes through both networks, and $O(252,205)$ operations for target network updates, yielding a total per-step complexity of $O(631,611)$ operations.

\textbf{TD3 per training step:}

Each TD3 training step requires $O(125,504)$ operations for the actor forward pass, $O(253,402)$ operations for forward passes through both critics, $O(253,402)$ operations for backward passes through both critics, $O(125,504/d)$ operations for the actor backward pass (performed every $d$ steps), and $O(378,906/d)$ operations for target network updates (performed every $d$ steps), resulting in a total amortized per-step complexity of $O(884,765)$ operations.

\textbf{Computational overhead:}
\begin{equation}
\text{TD3 overhead} = \frac{884,765 - 631,611}{631,611} \approx 40\%
\end{equation}

This aligns with observed training time increase (8 hours to 12 hours, or 50\% increase when accounting for longer episodes).

\subsubsection{Deployment Phase Complexity}

For real-time control (inference only):

\textbf{DDPG inference:}
\begin{equation}
a = \mu(s|\theta_\mu) \rightarrow O(125,504) \text{ operations}
\end{equation}

\textbf{TD3 inference:}
\begin{equation}
a = \mu(s|\theta_\mu) \rightarrow O(125,504) \text{ operations}
\end{equation}

\textbf{Critical insight:} Both algorithms have \textbf{identical inference complexity}. The twin critics are only used during training, not during real-time deployment. This means TD3's computational overhead during training does not impact real-time control performance.
\\
----------------tbd--------
\\
\subsubsection{Memory Requirements}
\label{subsec:memory_requirements}
\\
----------------tbd--------
\\
\begin{table}[htbp]
\centering
\caption{Memory requirements comparison}
\label{tab:memory_comparison}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Component} & \textbf{DDPG} & \textbf{TD3} \\
\hline
Actor network (online) & 502 KB & 502 KB \\
Actor network (target) & 502 KB & 502 KB \\
Critic network(s) (online) & 507 KB & 1.01 MB \\
Critic network(s) (target) & 507 KB & 1.01 MB \\
Replay buffer & 440 MB & 440 MB \\
\hline
\textbf{Total training memory} & \textbf{442 MB} & \textbf{443 MB} \\
\textbf{Total deployment memory} & \textbf{502 KB} & \textbf{502 KB} \\
\hline
\end{tabular}
\end{table}

The replay buffer dominates memory usage during training, making the difference between DDPG and TD3 negligible ($<$1\% increase). For deployment, memory requirements are identical.

\subsubsection{Hyperparameter Sensitivity}
\label{subsec:hyperparam_sensitivity}

Based on ablation studies conducted during development:

\textbf{DDPG Sensitivity:}

DDPG exhibits high sensitivity to actor and critic learning rates, requiring careful tuning for stable convergence. The algorithm shows moderate sensitivity to the target update rate $\tau$, while demonstrating low sensitivity to the Ornstein-Uhlenbeck noise parameters.

\textbf{TD3 Sensitivity:}

TD3 demonstrates moderate sensitivity to actor and critic learning rates, though less pronounced than DDPG. The algorithm exhibits low sensitivity to the policy delay parameter $d$ and target smoothing parameters, and very low sensitivity to the target update rate $\tau$, making it substantially more robust to hyperparameter variations.

\textbf{Practical implication:} TD3 is significantly more robust to hyperparameter choices, making it easier to deploy across different systems without extensive tuning.

\subsubsection{Practical Deployment Considerations}
\label{subsec:deployment_considerations}

\subsubsection{When to Choose DDPG}

\textbf{Recommended for:}

DDPG is particularly well-suited for proof-of-concept studies, applications where training time is critical and must be minimized, systems with well-characterized dynamics that can benefit from its simpler architecture, scenarios where slightly aggressive control behavior is acceptable, and situations with limited computational resources during the training phase.

\subsubsection{When to Choose TD3}

\textbf{Recommended for:}

TD3 is the preferred choice for production deployments in power systems, safety-critical applications where reliability is paramount, systems requiring high voltage stability with minimal oscillations, operation under uncertain or varying operating conditions, applications that are sensitive to control overshoot and require conservative action selection, and scenarios where training time is not a primary constraint and performance takes precedence.

\subsubsection{Real-Time Implementation Considerations}

Both algorithms are suitable for real-time control:

\textbf{Sampling time:} 1 ms (sufficient for both algorithms)

\textbf{Inference time:} $<$ 0.5 ms on modern DSP hardware

\textbf{Hardware requirements:}

The minimum hardware requirement for real-time implementation is a Texas Instruments TMS320F28379D digital signal processor (32-bit, 200 MHz), while the recommended platforms include the dSPACE MicroLabBox or OPAL-RT OP5707 for enhanced performance and flexibility.

\subsubsection{Summary and Recommendations}
\label{subsec:implementation_summary}

\textbf{Key insights:}

\begin{enumerate}
    \item \textbf{Performance vs. Cost Trade-off:} TD3 provides 2--10\% performance improvement (depending on metric) at 50\% additional training cost
    
    \item \textbf{No Real-Time Penalty:} Despite higher training complexity, TD3 has identical inference cost to DDPG, making the performance gains free during deployment
    
    \item \textbf{Robustness:} TD3's reduced hyperparameter sensitivity makes it more practical for real-world deployment where extensive tuning may not be feasible
    
    \item \textbf{Safety:} TD3's more conservative control actions make it preferable for power system applications where overshoot can cause equipment damage
\end{enumerate}

\textbf{Overall recommendation:} For the DFIG-Solar PV hybrid system, \textbf{TD3 is the preferred choice} for production deployment despite longer training time, due to its superior stability, robustness, and safety characteristics. DDPG remains valuable for rapid prototyping and initial development.

The quantitative performance comparison based on OPAL-RT HIL simulation results is presented in Chapter~\ref{chap:td3}.